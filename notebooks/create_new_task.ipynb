{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttktjmt/mjlab/blob/main/notebooks/create_new_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO76KS1i-MwA"
      },
      "source": [
        "# **ü§ñ CartPole Tutorial with MJLab**\n",
        "\n",
        "This notebook demonstrates how to create a custom reinforcement learning task using MJLab. We'll build a CartPole environment from scratch, including:\n",
        "\n",
        "1. **Robot Definition** - Define the CartPole model in MuJoCo XML\n",
        "2. **Task Configuration** - Set up observations, actions, rewards, and terminations\n",
        "3. **Training** - Train a policy using PPO\n",
        "4. **Evaluation** - Visualize/Record the trained policy\n",
        "\n",
        "> **Note**: This tutorial is created based on the official MJLab documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ywZTgfR3C_w"
      },
      "source": [
        "## **üì¶ Setup and Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dtLMJHzy3Nee",
        "outputId": "12c971d3-52ab-40e9-ac07-62a6da9ab80a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mjlab\n",
            "‚úì Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Clone the mjlab repository\n",
        "!if [ ! -d \"mjlab\" ]; then git clone -q https://github.com/mujocolab/mjlab.git; fi\n",
        "%cd /content/mjlab\n",
        "\n",
        "# Install mjlab in editable mode\n",
        "!uv pip install --system -e . -q\n",
        "\n",
        "print(\"‚úì Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSf2943z3b0s"
      },
      "source": [
        "### **üîë WandB Setup (Optional)**\n",
        "\n",
        "Configure Weights & Biases for experiment tracking. Add your WandB API key to Colab Secrets:\n",
        "- `WANDB_API_KEY`: from [wandb.ai/authorize](https://wandb.ai/authorize)\n",
        "- `WANDB_ENTITY`: your wandb entity name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC9ywCnm3dGg",
        "outputId": "3ce599e0-fdfc-428f-b062-18ba1f1533a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì WandB configured successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    # Set this to disable wandb logger\n",
        "    # os.environ['WANDB_MODE'] = 'disabled'\n",
        "\n",
        "    # Set this to use wandb logger\n",
        "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "    os.environ['WANDB_ENTITY'] = userdata.get('WANDB_ENTITY')\n",
        "\n",
        "    print(\"‚úì WandB configured successfully!\")\n",
        "except (AttributeError, KeyError):\n",
        "    print(\"‚ö† WandB secrets not found. Training will proceed without WandB logging.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mispfmy73lmq"
      },
      "source": [
        "---\n",
        "\n",
        "## **ü§ñ Step 1: Define the Robot**\n",
        "\n",
        "We'll create a simple CartPole robot with:\n",
        "- A sliding cart (1 DOF)\n",
        "- A hinged pole (1 DOF)\n",
        "- A velocity actuator to control the cart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FvJYPWD3scd"
      },
      "source": [
        "### **üìÅ Structure Directories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP-yET-R3ofN",
        "outputId": "bad99727-60b8-4a97-dca0-bbc6c008e7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Directory structure created\n"
          ]
        }
      ],
      "source": [
        "# Create the cartpole robot directory structure\n",
        "!mkdir -p /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/\n",
        "!mkdir -p /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/xmls\n",
        "\n",
        "print(\"‚úì Directory structure created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRyN1Pok3u25"
      },
      "source": [
        "### **üìù Create MuJoCo XML Model**\n",
        "\n",
        "This XML defines the CartPole physics:\n",
        "- **Ground plane** for visualization\n",
        "- **Cart body** with a sliding joint (¬±2m range)\n",
        "- **Pole body** with a hinge joint (¬±90¬∞ range)\n",
        "- **Velocity actuator** for cart control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWGyFX5V3yWc",
        "outputId": "67665ae9-5310-4c47-d9a9-012a2c5d4334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/xmls/cartpole.xml\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/xmls/cartpole.xml\n",
        "<mujoco model=\"cartpole\">\n",
        "  <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\"/>\n",
        "  <worldbody>\n",
        "    <geom name=\"ground\" type=\"plane\" pos=\"0 0 0\" size=\"5 5 0.1\" rgba=\"0.8 0.9 0.8 1\"/>\n",
        "    <body name=\"cart\" pos=\"0 0 0.1\">\n",
        "      <geom type=\"box\" size=\"0.2 0.1 0.1\" rgba=\"0.2 0.2 0.8 1\" mass=\"1.0\"/>\n",
        "      <joint name=\"slide\" type=\"slide\" axis=\"1 0 0\" limited=\"true\" range=\"-2 2\"/>\n",
        "      <body name=\"pole\" pos=\"0 0 0.1\">\n",
        "        <geom type=\"capsule\" size=\"0.05 0.5\" fromto=\"0 0 0 0 0 1\" rgba=\"0.8 0.2 0.2 1\" mass=\"2.0\"/>\n",
        "        <joint name=\"hinge\" type=\"hinge\" axis=\"0 1 0\" range=\"-90 90\"/>\n",
        "      </body>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <velocity name=\"slide_velocity\" joint=\"slide\" ctrlrange=\"-20 20\" kv=\"20\"/>\n",
        "  </actuator>\n",
        "</mujoco>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpYCG9jI31dZ"
      },
      "source": [
        "### **‚öôÔ∏è Create Robot Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDhiyDTn4AVa",
        "outputId": "dc54509c-e42d-4701-f196-0b2e10485e4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/cartpole_constants.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/cartpole_constants.py\n",
        "from pathlib import Path\n",
        "import mujoco\n",
        "\n",
        "from mjlab import MJLAB_SRC_PATH\n",
        "from mjlab.entity import Entity, EntityCfg, EntityArticulationInfoCfg\n",
        "from mjlab.actuator import XmlVelocityActuatorCfg\n",
        "\n",
        "CARTPOLE_XML: Path = (\n",
        "  MJLAB_SRC_PATH / \"asset_zoo\" / \"robots\" / \"cartpole\" / \"xmls\" / \"cartpole.xml\"\n",
        ")\n",
        "assert CARTPOLE_XML.exists(), f\"XML not found: {CARTPOLE_XML}\"\n",
        "\n",
        "def get_spec() -> mujoco.MjSpec:\n",
        "  return mujoco.MjSpec.from_file(str(CARTPOLE_XML))\n",
        "\n",
        "def get_cartpole_robot_cfg() -> EntityCfg:\n",
        "  \"\"\"Get a fresh CartPole robot configuration instance.\"\"\"\n",
        "  actuators = (\n",
        "    XmlVelocityActuatorCfg(\n",
        "      joint_names_expr=(\"slide\",),\n",
        "    ),\n",
        "  )\n",
        "  articulation = EntityArticulationInfoCfg(actuators=actuators)\n",
        "  return EntityCfg(\n",
        "    spec_fn=get_spec,\n",
        "    articulation=articulation\n",
        "  )\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   import mujoco.viewer as viewer\n",
        "#   robot = Entity(get_cartpole_robot_cfg())\n",
        "#   viewer.launch(robot.spec.compile())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WSaDod04FwN",
        "outputId": "af92a1a8-116a-4560-8f63-d9b71e0158aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Create __init__.py for the cartpole robot package\n",
        "%%writefile /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/__init__.py\n",
        "# Empty __init__.py to mark the directory as a Python package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1tiBPfp_oVP",
        "outputId": "833cade0-367f-4666-fd03-f8144dbf2e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Added /content/mjlab/src to Python path\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Append src dir to python path\n",
        "mjlab_src = '/content/mjlab/src'\n",
        "if mjlab_src not in sys.path:\n",
        "    sys.path.insert(0, mjlab_src)\n",
        "    print(f\"‚úì Added {mjlab_src} to Python path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToWF84qC4Hfg"
      },
      "source": [
        "### **‚úÖ Verify Robot Setup**\n",
        "\n",
        "Let's test that the robot can be loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tVsvqzQ4J9h",
        "outputId": "22d9a13f-b2ff-4a0e-a2da-0644867fcb8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì CartPole robot loaded successfully!\n",
            "  ‚Ä¢ Degrees of Freedom (DOF): 2\n",
            "  ‚Ä¢ Number of Actuators: 1\n",
            "  ‚Ä¢ Bodies: 4\n",
            "  ‚Ä¢ Joints: 2\n"
          ]
        }
      ],
      "source": [
        "from mjlab.entity import Entity\n",
        "from mjlab.asset_zoo.robots.cartpole.cartpole_constants import get_cartpole_robot_cfg\n",
        "\n",
        "# Load the robot\n",
        "robot = Entity(get_cartpole_robot_cfg())\n",
        "model = robot.spec.compile()\n",
        "\n",
        "# Display robot information\n",
        "print(\"‚úì CartPole robot loaded successfully!\")\n",
        "print(f\"  ‚Ä¢ Degrees of Freedom (DOF): {model.nv}\")\n",
        "print(f\"  ‚Ä¢ Number of Actuators: {model.nu}\")\n",
        "print(f\"  ‚Ä¢ Bodies: {model.nbody}\")\n",
        "print(f\"  ‚Ä¢ Joints: {model.njnt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2_9dixlHON1"
      },
      "source": [
        "### **üìã Register the Robot**\n",
        "\n",
        "Add the CartPole robot to the asset zoo registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qDIF__lHPcb",
        "outputId": "70b0afde-c607-4212-b043-d46c4cedaa54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì CartPole robot registered in asset zoo\n"
          ]
        }
      ],
      "source": [
        "# Add CartPole import to robots __init__.py\n",
        "with open('/content/mjlab/src/mjlab/asset_zoo/robots/__init__.py', 'a') as f:\n",
        "    f.write('\\n# CartPole robot\\n')\n",
        "    f.write('from mjlab.asset_zoo.robots.cartpole.cartpole_constants import ')\n",
        "    f.write('get_cartpole_robot_cfg as get_cartpole_robot_cfg\\n')\n",
        "\n",
        "print(\"‚úì CartPole robot registered in asset zoo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lVD_L6PHWNm"
      },
      "source": [
        "---\n",
        "\n",
        "## **üéØ Step 2: Define the Task (MDP)**\n",
        "\n",
        "Now we'll define the Markov Decision Process:\n",
        "- **Observations**: pole angle, angular velocity, cart position, cart velocity\n",
        "- **Actions**: cart velocity commands\n",
        "- **Rewards**: upright reward + effort penalty\n",
        "- **Terminations**: pole tips over or timeout\n",
        "- **Events**: random pushes for robustness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxe4TBrHb-I"
      },
      "source": [
        "### **üìÅ Create Task Directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWBqdkziHc2G",
        "outputId": "5971ee93-cd52-4c84-d2c7-3af2299e5990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Task directory created\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/mjlab/src/mjlab/tasks/cartpole\n",
        "\n",
        "print(\"‚úì Task directory created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJfjPpm0Hhj1"
      },
      "source": [
        "### **üìù Create Environment Configuration**\n",
        "\n",
        "This file contains the MDP (Markov Decision Process) components:\n",
        "1. **Scene Config**: 64 parallel environments\n",
        "2. **Actions**: Joint velocity control with 20.0 scale\n",
        "3. **Observations**: Normalized state variables\n",
        "4. **Rewards**: Upright reward (5.0) + effort penalty (-0.01)\n",
        "5. **Events**: Joint resets + random pushes\n",
        "6. **Terminations**: Pole tipped (>30¬∞) or timeout (10s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "javx9XDIHkFI",
        "outputId": "1827e08c-e555-4979-ab36-b28fb231edb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/tasks/cartpole/env_cfg.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/tasks/cartpole/env_cfg.py\n",
        "\"\"\"CartPole task environment configuration.\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from mjlab.envs import ManagerBasedRlEnvCfg\n",
        "from mjlab.envs.mdp.actions import JointVelocityActionCfg\n",
        "from mjlab.managers.manager_term_config import (\n",
        "  ObservationGroupCfg,\n",
        "  ObservationTermCfg,\n",
        "  RewardTermCfg,\n",
        "  TerminationTermCfg,\n",
        "  EventTermCfg,\n",
        ")\n",
        "from mjlab.managers.scene_entity_config import SceneEntityCfg\n",
        "from mjlab.scene import SceneCfg\n",
        "from mjlab.sim import MujocoCfg, SimulationCfg\n",
        "from mjlab.viewer import ViewerConfig\n",
        "from mjlab.asset_zoo.robots.cartpole.cartpole_constants import get_cartpole_robot_cfg\n",
        "from mjlab.envs import mdp\n",
        "\n",
        "\n",
        "def cartpole_env_cfg(play: bool = False) -> ManagerBasedRlEnvCfg:\n",
        "  \"\"\"Create CartPole environment configuration.\n",
        "\n",
        "  Args:\n",
        "    play: If True, disables corruption and extends episode length for evaluation.\n",
        "  \"\"\"\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Scene Configuration\n",
        "  # ==============================================================================\n",
        "\n",
        "  scene_cfg = SceneCfg(\n",
        "    num_envs=64 if not play else 16,  # Fewer envs for play mode\n",
        "    extent=1.0,   # Spacing between environments\n",
        "    entities={\"robot\": get_cartpole_robot_cfg()},\n",
        "  )\n",
        "\n",
        "  viewer_cfg = ViewerConfig(\n",
        "    origin_type=ViewerConfig.OriginType.ASSET_BODY,\n",
        "    asset_name=\"robot\",\n",
        "    body_name=\"pole\",\n",
        "    distance=3.0,\n",
        "    elevation=10.0,\n",
        "    azimuth=90.0,\n",
        "  )\n",
        "\n",
        "  sim_cfg = SimulationCfg(\n",
        "    mujoco=MujocoCfg(\n",
        "      timestep=0.02,  # 50 Hz control\n",
        "      iterations=1,\n",
        "    ),\n",
        "  )\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Actions\n",
        "  # ==============================================================================\n",
        "\n",
        "  actions = {\n",
        "    \"joint_pos\": JointVelocityActionCfg(\n",
        "      asset_name=\"robot\",\n",
        "      actuator_names=(\".*\",),\n",
        "      scale=20.0,\n",
        "      use_default_offset=False,\n",
        "    ),\n",
        "  }\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Observations\n",
        "  # ==============================================================================\n",
        "\n",
        "  policy_terms = {\n",
        "    \"angle\": ObservationTermCfg(\n",
        "      func=lambda env: env.sim.data.qpos[:, 1:2] / math.pi\n",
        "    ),\n",
        "    \"ang_vel\": ObservationTermCfg(\n",
        "      func=lambda env: env.sim.data.qvel[:, 1:2] / 5.0\n",
        "    ),\n",
        "    \"cart_pos\": ObservationTermCfg(\n",
        "      func=lambda env: env.sim.data.qpos[:, 0:1] / 2.0\n",
        "    ),\n",
        "    \"cart_vel\": ObservationTermCfg(\n",
        "      func=lambda env: env.sim.data.qvel[:, 0:1] / 20.0\n",
        "    ),\n",
        "  }\n",
        "\n",
        "  observations = {\n",
        "    \"policy\": ObservationGroupCfg(\n",
        "      terms=policy_terms,\n",
        "      concatenate_terms=True,\n",
        "      enable_corruption=not play,  # Disable corruption in play mode\n",
        "    ),\n",
        "    \"critic\": ObservationGroupCfg(\n",
        "      terms=policy_terms,  # Critic uses same observations\n",
        "      concatenate_terms=True,\n",
        "      enable_corruption=False,\n",
        "    ),\n",
        "  }\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Rewards\n",
        "  # ==============================================================================\n",
        "\n",
        "  def compute_upright_reward(env):\n",
        "    \"\"\"Reward for keeping pole upright (cosine of angle).\"\"\"\n",
        "    return env.sim.data.qpos[:, 1].cos()\n",
        "\n",
        "  def compute_effort_penalty(env):\n",
        "    \"\"\"Penalty for control effort.\"\"\"\n",
        "    return -0.01 * (env.sim.data.ctrl[:, 0] ** 2)\n",
        "\n",
        "  rewards = {\n",
        "    \"upright\": RewardTermCfg(func=compute_upright_reward, weight=5.0),\n",
        "    \"effort\": RewardTermCfg(func=compute_effort_penalty, weight=1.0),\n",
        "  }\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Events\n",
        "  # ==============================================================================\n",
        "\n",
        "  def random_push_cart(env, env_ids, force_range=(-5, 5)):\n",
        "    \"\"\"Apply random force to cart for robustness training.\"\"\"\n",
        "    n = len(env_ids)\n",
        "    random_forces = (\n",
        "      torch.rand(n, device=env.device) *\n",
        "      (force_range[1] - force_range[0]) +\n",
        "      force_range[0]\n",
        "    )\n",
        "    env.sim.data.qfrc_applied[env_ids, 0] = random_forces\n",
        "\n",
        "  events = {\n",
        "    \"reset_robot_joints\": EventTermCfg(\n",
        "      func=mdp.reset_joints_by_offset,\n",
        "      mode=\"reset\",\n",
        "      params={\n",
        "        \"asset_cfg\": SceneEntityCfg(\"robot\"),\n",
        "        \"position_range\": (-0.1, 0.1),\n",
        "        \"velocity_range\": (-0.1, 0.1),\n",
        "      },\n",
        "    ),\n",
        "  }\n",
        "\n",
        "  # Add random pushes only in training mode\n",
        "  if not play:\n",
        "    events[\"random_push\"] = EventTermCfg(\n",
        "      func=random_push_cart,\n",
        "      mode=\"interval\",\n",
        "      interval_range_s=(1.0, 2.0),\n",
        "      params={\"force_range\": (-20.0, 20.0)},\n",
        "    )\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Terminations\n",
        "  # ==============================================================================\n",
        "\n",
        "  def check_pole_tipped(env):\n",
        "    \"\"\"Check if pole has tipped beyond 30 degrees.\"\"\"\n",
        "    return env.sim.data.qpos[:, 1].abs() > math.radians(30)\n",
        "\n",
        "  terminations = {\n",
        "    \"timeout\": TerminationTermCfg(func=mdp.time_out, time_out=True),\n",
        "    \"tipped\": TerminationTermCfg(func=check_pole_tipped, time_out=False),\n",
        "  }\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Environment Configuration\n",
        "  # ==============================================================================\n",
        "\n",
        "  return ManagerBasedRlEnvCfg(\n",
        "    scene=scene_cfg,\n",
        "    observations=observations,\n",
        "    actions=actions,\n",
        "    rewards=rewards,\n",
        "    events=events,\n",
        "    terminations=terminations,\n",
        "    sim=sim_cfg,\n",
        "    viewer=viewer_cfg,\n",
        "    decimation=1,           # No action repeat\n",
        "    episode_length_s=int(1e9) if play else 10.0,  # Infinite for play, 10s for training\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC5maMjzSj_X"
      },
      "source": [
        "### **‚öôÔ∏è Create RL Configuration**\n",
        "\n",
        "This file defines the PPO (Proximal Policy Optimization) training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C81zZm6mSj_X",
        "outputId": "002b27cf-8071-4105-e8e7-f4e5286bff22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/tasks/cartpole/rl_cfg.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/tasks/cartpole/rl_cfg.py\n",
        "\"\"\"RL configuration for CartPole task.\"\"\"\n",
        "\n",
        "from mjlab.rl.config import (\n",
        "  RslRlOnPolicyRunnerCfg,\n",
        "  RslRlPpoActorCriticCfg,\n",
        "  RslRlPpoAlgorithmCfg,\n",
        ")\n",
        "\n",
        "\n",
        "def cartpole_ppo_runner_cfg() -> RslRlOnPolicyRunnerCfg:\n",
        "  \"\"\"Create RL runner configuration for CartPole task.\"\"\"\n",
        "  return RslRlOnPolicyRunnerCfg(\n",
        "    policy=RslRlPpoActorCriticCfg(\n",
        "      init_noise_std=1.0,\n",
        "      actor_obs_normalization=True,\n",
        "      critic_obs_normalization=True,\n",
        "      actor_hidden_dims=(256, 128, 64),  # Smaller network for simpler task\n",
        "      critic_hidden_dims=(256, 128, 64),\n",
        "      activation=\"elu\",\n",
        "    ),\n",
        "    algorithm=RslRlPpoAlgorithmCfg(\n",
        "      value_loss_coef=1.0,\n",
        "      use_clipped_value_loss=True,\n",
        "      clip_param=0.2,\n",
        "      entropy_coef=0.01,\n",
        "      num_learning_epochs=5,\n",
        "      num_mini_batches=4,\n",
        "      learning_rate=1.0e-3,\n",
        "      schedule=\"adaptive\",\n",
        "      gamma=0.99,\n",
        "      lam=0.95,\n",
        "      desired_kl=0.01,\n",
        "      max_grad_norm=1.0,\n",
        "    ),\n",
        "    experiment_name=\"cartpole\",\n",
        "    save_interval=50,\n",
        "    num_steps_per_env=24,\n",
        "    max_iterations=5_000,  # Fewer iterations for simpler task\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc8-AHGcHt78"
      },
      "source": [
        "### **üìã Register the Task Environment**\n",
        "\n",
        "Register the CartPole task with mjlab registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YitUGUBRHxD4",
        "outputId": "88e906b5-3534-4e02-93c8-e5482e8af0d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/tasks/cartpole/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/tasks/cartpole/__init__.py\n",
        "\"\"\"CartPole task registration.\"\"\"\n",
        "\n",
        "from mjlab.tasks.registry import register_mjlab_task\n",
        "from mjlab.tasks.velocity.rl import VelocityOnPolicyRunner\n",
        "\n",
        "from .env_cfg import cartpole_env_cfg\n",
        "from .rl_cfg import cartpole_ppo_runner_cfg\n",
        "\n",
        "register_mjlab_task(\n",
        "  task_id=\"Mjlab-Cartpole\",\n",
        "  env_cfg=cartpole_env_cfg(),\n",
        "  play_env_cfg=cartpole_env_cfg(play=True),\n",
        "  rl_cfg=cartpole_ppo_runner_cfg(),\n",
        "  runner_cls=VelocityOnPolicyRunner,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7wqLZR1rnGn"
      },
      "source": [
        "---\n",
        "\n",
        "## **üöÄ Step 3: Train the Agent**\n",
        "\n",
        "Now let's train a PPO policy to balance the CartPole!\n",
        "\n",
        "**Training Configuration:**\n",
        "- Algorithm: PPO (Proximal Policy Optimization)\n",
        "- Parallel Environments: 64\n",
        "- Episode Length: 10 seconds (500 steps @ 50Hz)\n",
        "- Total Steps: ~5-10 million (adjust as needed)\n",
        "\n",
        "**‚ö†Ô∏è You may need to create a project named \"mjlab\" on wandb UI manually, since google colab doesn't have permission to create a new project.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hht_hF4trqP2",
        "outputId": "26de85b9-3b00-4c39-b078-2dbad0e91617"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "                           Time elapsed: 00:02:57\n",
            "                                    ETA: 00:00:52\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 773/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1188864 \n",
            "                       Steps per second: 6822 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: -4.5735\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:02:57\n",
            "                                    ETA: 00:00:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 774/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1190400 \n",
            "                       Steps per second: 5406 \n",
            "                        Collection time: 0.113s \n",
            "                          Learning time: 0.171s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0031\n",
            "                      Mean entropy loss: -4.4930\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0032\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:02:57\n",
            "                                    ETA: 00:00:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 775/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1191936 \n",
            "                       Steps per second: 5431 \n",
            "                        Collection time: 0.142s \n",
            "                          Learning time: 0.140s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0015\n",
            "                      Mean entropy loss: -4.5210\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:02:58\n",
            "                                    ETA: 00:00:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 776/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1193472 \n",
            "                       Steps per second: 5809 \n",
            "                        Collection time: 0.126s \n",
            "                          Learning time: 0.138s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0102\n",
            "                      Mean entropy loss: -4.4786\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:02:58\n",
            "                                    ETA: 00:00:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 777/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1195008 \n",
            "                       Steps per second: 5593 \n",
            "                        Collection time: 0.140s \n",
            "                          Learning time: 0.135s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0013\n",
            "                      Mean entropy loss: -4.4373\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:02:58\n",
            "                                    ETA: 00:00:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 778/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1196544 \n",
            "                       Steps per second: 5449 \n",
            "                        Collection time: 0.120s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0106\n",
            "                      Mean entropy loss: -4.4420\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0059\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:02:58\n",
            "                                    ETA: 00:00:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 779/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1198080 \n",
            "                       Steps per second: 5646 \n",
            "                        Collection time: 0.129s \n",
            "                          Learning time: 0.143s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0018\n",
            "                      Mean entropy loss: -4.5219\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:02:59\n",
            "                                    ETA: 00:00:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 780/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1199616 \n",
            "                       Steps per second: 5805 \n",
            "                        Collection time: 0.117s \n",
            "                          Learning time: 0.147s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0023\n",
            "                      Mean entropy loss: -4.5293\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:02:59\n",
            "                                    ETA: 00:00:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 781/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1201152 \n",
            "                       Steps per second: 5039 \n",
            "                        Collection time: 0.145s \n",
            "                          Learning time: 0.160s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0005\n",
            "                      Mean entropy loss: -4.6255\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:02:59\n",
            "                                    ETA: 00:00:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 782/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1202688 \n",
            "                       Steps per second: 4522 \n",
            "                        Collection time: 0.158s \n",
            "                          Learning time: 0.182s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: -4.7102\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9995\n",
            "                  Episode_Reward/effort: -0.0059\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.34s\n",
            "                           Time elapsed: 00:03:00\n",
            "                                    ETA: 00:00:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 783/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1204224 \n",
            "                       Steps per second: 6849 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.115s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0040\n",
            "                      Mean entropy loss: -4.6617\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0030\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:00\n",
            "                                    ETA: 00:00:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 784/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1205760 \n",
            "                       Steps per second: 6842 \n",
            "                        Collection time: 0.112s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0021\n",
            "                      Mean entropy loss: -4.6182\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:00\n",
            "                                    ETA: 00:00:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 785/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1207296 \n",
            "                       Steps per second: 7055 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0039\n",
            "                      Mean entropy loss: -4.5347\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:00\n",
            "                                    ETA: 00:00:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 786/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1208832 \n",
            "                       Steps per second: 6691 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.129s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0018\n",
            "                      Mean entropy loss: -4.6424\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:00\n",
            "                                    ETA: 00:00:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 787/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1210368 \n",
            "                       Steps per second: 7414 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0031\n",
            "                      Mean entropy loss: -4.6284\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:01\n",
            "                                    ETA: 00:00:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 788/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1211904 \n",
            "                       Steps per second: 7224 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: -4.6368\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 5.0000\n",
            "                  Episode_Reward/effort: -0.0014\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:01\n",
            "                                    ETA: 00:00:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 789/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1213440 \n",
            "                       Steps per second: 7631 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0088\n",
            "                      Mean entropy loss: -4.6818\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 5.0000\n",
            "                  Episode_Reward/effort: -0.0014\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:01\n",
            "                                    ETA: 00:00:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 790/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1214976 \n",
            "                       Steps per second: 7443 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0087\n",
            "                      Mean entropy loss: -4.7439\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0046\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:01\n",
            "                                    ETA: 00:00:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 791/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1216512 \n",
            "                       Steps per second: 7127 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0038\n",
            "                      Mean entropy loss: -4.7456\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0039\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:02\n",
            "                                    ETA: 00:00:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 792/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1218048 \n",
            "                       Steps per second: 7259 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0078\n",
            "                      Mean entropy loss: -4.8330\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0031\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:02\n",
            "                                    ETA: 00:00:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 793/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1219584 \n",
            "                       Steps per second: 7504 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0151\n",
            "                      Mean entropy loss: -4.8474\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0031\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:02\n",
            "                                    ETA: 00:00:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 794/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1221120 \n",
            "                       Steps per second: 7201 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0102\n",
            "                      Mean entropy loss: -4.8493\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0028\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:02\n",
            "                                    ETA: 00:00:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 795/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1222656 \n",
            "                       Steps per second: 6695 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.130s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0018\n",
            "                      Mean entropy loss: -4.8728\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9995\n",
            "                  Episode_Reward/effort: -0.0054\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:02\n",
            "                                    ETA: 00:00:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 796/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1224192 \n",
            "                       Steps per second: 7327 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0059\n",
            "                      Mean entropy loss: -4.8680\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:03\n",
            "                                    ETA: 00:00:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 797/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1225728 \n",
            "                       Steps per second: 6805 \n",
            "                        Collection time: 0.113s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: -4.8403\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:03\n",
            "                                    ETA: 00:00:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 798/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1227264 \n",
            "                       Steps per second: 7095 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0016\n",
            "                      Mean entropy loss: -4.8314\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:03\n",
            "                                    ETA: 00:00:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 799/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1228800 \n",
            "                       Steps per second: 7294 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0103\n",
            "                      Mean entropy loss: -4.7613\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:03\n",
            "                                    ETA: 00:00:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 800/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1230336 \n",
            "                       Steps per second: 6457 \n",
            "                        Collection time: 0.124s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0043\n",
            "                      Mean entropy loss: -4.7362\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:03:03\n",
            "                                    ETA: 00:00:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 801/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1231872 \n",
            "                       Steps per second: 7094 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.116s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0043\n",
            "                      Mean entropy loss: -4.7514\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:04\n",
            "                                    ETA: 00:00:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 802/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1233408 \n",
            "                       Steps per second: 7068 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0017\n",
            "                      Mean entropy loss: -4.6759\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:04\n",
            "                                    ETA: 00:00:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 803/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1234944 \n",
            "                       Steps per second: 6962 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: -4.7698\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:04\n",
            "                                    ETA: 00:00:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 804/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1236480 \n",
            "                       Steps per second: 6136 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.143s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0025\n",
            "                      Mean entropy loss: -4.7321\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:03:04\n",
            "                                    ETA: 00:00:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 805/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1238016 \n",
            "                       Steps per second: 7043 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: -4.6993\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0032\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:05\n",
            "                                    ETA: 00:00:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 806/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1239552 \n",
            "                       Steps per second: 6680 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0060\n",
            "                      Mean entropy loss: -4.6918\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:05\n",
            "                                    ETA: 00:00:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 807/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1241088 \n",
            "                       Steps per second: 7191 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -4.6196\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:05\n",
            "                                    ETA: 00:00:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 808/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1242624 \n",
            "                       Steps per second: 7255 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0006\n",
            "                      Mean entropy loss: -4.6293\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:05\n",
            "                                    ETA: 00:00:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 809/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1244160 \n",
            "                       Steps per second: 7280 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0047\n",
            "                      Mean entropy loss: -4.5472\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0051\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:05\n",
            "                                    ETA: 00:00:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 810/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1245696 \n",
            "                       Steps per second: 7570 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: -4.5898\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:06\n",
            "                                    ETA: 00:00:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 811/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1247232 \n",
            "                       Steps per second: 7738 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0010\n",
            "                      Mean entropy loss: -4.5435\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:06\n",
            "                                    ETA: 00:00:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 812/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1248768 \n",
            "                       Steps per second: 7360 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0037\n",
            "                      Mean entropy loss: -4.5585\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0056\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:06\n",
            "                                    ETA: 00:00:42\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 813/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1250304 \n",
            "                       Steps per second: 7195 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.121s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0007\n",
            "                      Mean entropy loss: -4.5236\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0060\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:06\n",
            "                                    ETA: 00:00:42\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 814/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1251840 \n",
            "                       Steps per second: 7507 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0180\n",
            "                      Mean entropy loss: -4.6156\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0060\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:07\n",
            "                                    ETA: 00:00:42\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 815/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1253376 \n",
            "                       Steps per second: 7267 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: -4.6149\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0032\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:07\n",
            "                                    ETA: 00:00:42\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 816/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1254912 \n",
            "                       Steps per second: 7243 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0233\n",
            "                      Mean entropy loss: -4.6144\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:07\n",
            "                                    ETA: 00:00:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 817/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1256448 \n",
            "                       Steps per second: 7340 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0070\n",
            "                      Mean entropy loss: -4.4929\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:07\n",
            "                                    ETA: 00:00:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 818/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1257984 \n",
            "                       Steps per second: 6733 \n",
            "                        Collection time: 0.117s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: -4.5460\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:07\n",
            "                                    ETA: 00:00:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 819/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1259520 \n",
            "                       Steps per second: 7145 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.115s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0010\n",
            "                      Mean entropy loss: -4.5218\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0032\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:08\n",
            "                                    ETA: 00:00:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 820/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1261056 \n",
            "                       Steps per second: 7231 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0156\n",
            "                      Mean entropy loss: -4.5048\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:08\n",
            "                                    ETA: 00:00:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 821/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1262592 \n",
            "                       Steps per second: 7070 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0005\n",
            "                      Mean entropy loss: -4.5094\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9996\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:08\n",
            "                                    ETA: 00:00:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 822/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1264128 \n",
            "                       Steps per second: 6743 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.128s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0119\n",
            "                      Mean entropy loss: -4.4804\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:08\n",
            "                                    ETA: 00:00:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 823/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1265664 \n",
            "                       Steps per second: 7140 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0062\n",
            "                      Mean entropy loss: -4.4995\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0051\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:08\n",
            "                                    ETA: 00:00:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 824/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1267200 \n",
            "                       Steps per second: 6853 \n",
            "                        Collection time: 0.113s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: -4.4795\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:09\n",
            "                                    ETA: 00:00:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 825/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1268736 \n",
            "                       Steps per second: 6857 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0019\n",
            "                      Mean entropy loss: -4.4887\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:09\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 826/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1270272 \n",
            "                       Steps per second: 5873 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: -4.4935\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:03:09\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 827/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1271808 \n",
            "                       Steps per second: 5155 \n",
            "                        Collection time: 0.148s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0022\n",
            "                      Mean entropy loss: -4.5492\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0051\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:03:09\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 828/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1273344 \n",
            "                       Steps per second: 5776 \n",
            "                        Collection time: 0.121s \n",
            "                          Learning time: 0.145s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0016\n",
            "                      Mean entropy loss: -4.5277\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0039\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:10\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 829/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1274880 \n",
            "                       Steps per second: 5476 \n",
            "                        Collection time: 0.126s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0039\n",
            "                      Mean entropy loss: -4.5341\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:03:10\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 830/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1276416 \n",
            "                       Steps per second: 5640 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: -4.4902\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:10\n",
            "                                    ETA: 00:00:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 831/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1277952 \n",
            "                       Steps per second: 5709 \n",
            "                        Collection time: 0.125s \n",
            "                          Learning time: 0.144s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: -4.5653\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:11\n",
            "                                    ETA: 00:00:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 832/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1279488 \n",
            "                       Steps per second: 5913 \n",
            "                        Collection time: 0.123s \n",
            "                          Learning time: 0.137s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0004\n",
            "                      Mean entropy loss: -4.5065\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0039\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:03:11\n",
            "                                    ETA: 00:00:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 833/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1281024 \n",
            "                       Steps per second: 5407 \n",
            "                        Collection time: 0.123s \n",
            "                          Learning time: 0.161s \n",
            "                        Mean value loss: 0.0008\n",
            "                    Mean surrogate loss: 0.0053\n",
            "                      Mean entropy loss: -4.5491\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0025\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:03:11\n",
            "                                    ETA: 00:00:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 834/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1282560 \n",
            "                       Steps per second: 5150 \n",
            "                        Collection time: 0.118s \n",
            "                          Learning time: 0.180s \n",
            "                        Mean value loss: 0.0009\n",
            "                    Mean surrogate loss: 0.0237\n",
            "                      Mean entropy loss: -4.5402\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0025\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:03:11\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 835/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1284096 \n",
            "                       Steps per second: 5402 \n",
            "                        Collection time: 0.139s \n",
            "                          Learning time: 0.145s \n",
            "                        Mean value loss: 0.0003\n",
            "                    Mean surrogate loss: 0.0035\n",
            "                      Mean entropy loss: -4.5081\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0026\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:03:12\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 836/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1285632 \n",
            "                       Steps per second: 7352 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: 0.0039\n",
            "                      Mean entropy loss: -4.4434\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0030\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:12\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 837/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1287168 \n",
            "                       Steps per second: 7645 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0003\n",
            "                    Mean surrogate loss: 0.0069\n",
            "                      Mean entropy loss: -4.4096\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:12\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 838/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1288704 \n",
            "                       Steps per second: 6810 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.121s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0058\n",
            "                      Mean entropy loss: -4.4080\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:12\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 839/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1290240 \n",
            "                       Steps per second: 7216 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0022\n",
            "                      Mean entropy loss: -4.4053\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:13\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 840/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1291776 \n",
            "                       Steps per second: 7122 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0006\n",
            "                      Mean entropy loss: -4.3269\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:13\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 841/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1293312 \n",
            "                       Steps per second: 7275 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0004\n",
            "                    Mean surrogate loss: 0.0168\n",
            "                      Mean entropy loss: -4.3237\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0031\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:13\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 842/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1294848 \n",
            "                       Steps per second: 7204 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: 0.0047\n",
            "                      Mean entropy loss: -4.3834\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:13\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 843/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1296384 \n",
            "                       Steps per second: 7014 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: 0.0052\n",
            "                      Mean entropy loss: -4.3287\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:13\n",
            "                                    ETA: 00:00:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 844/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1297920 \n",
            "                       Steps per second: 6917 \n",
            "                        Collection time: 0.113s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0020\n",
            "                      Mean entropy loss: -4.4092\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:14\n",
            "                                    ETA: 00:00:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 845/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1299456 \n",
            "                       Steps per second: 7230 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0093\n",
            "                      Mean entropy loss: -4.3182\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:14\n",
            "                                    ETA: 00:00:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 846/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1300992 \n",
            "                       Steps per second: 6578 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.118s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0036\n",
            "                      Mean entropy loss: -4.3253\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:14\n",
            "                                    ETA: 00:00:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 847/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1302528 \n",
            "                       Steps per second: 6579 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.128s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: -4.3344\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:14\n",
            "                                    ETA: 00:00:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 848/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1304064 \n",
            "                       Steps per second: 7130 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0064\n",
            "                      Mean entropy loss: -4.3394\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0046\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:15\n",
            "                                    ETA: 00:00:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 849/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1305600 \n",
            "                       Steps per second: 7211 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0045\n",
            "                      Mean entropy loss: -4.3034\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0051\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:15\n",
            "                                    ETA: 00:00:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 850/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1307136 \n",
            "                       Steps per second: 7379 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: -4.3473\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0061\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:15\n",
            "                                    ETA: 00:00:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 851/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1308672 \n",
            "                       Steps per second: 7687 \n",
            "                        Collection time: 0.092s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0010\n",
            "                      Mean entropy loss: -4.2390\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9996\n",
            "                  Episode_Reward/effort: -0.0068\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:15\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 852/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1310208 \n",
            "                       Steps per second: 6996 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0267\n",
            "                      Mean entropy loss: -4.3285\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:15\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 853/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1311744 \n",
            "                       Steps per second: 7626 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -4.3185\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:16\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 854/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1313280 \n",
            "                       Steps per second: 7142 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.115s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: -4.3183\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0027\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:16\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 855/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1314816 \n",
            "                       Steps per second: 7684 \n",
            "                        Collection time: 0.092s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: -4.3353\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0027\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:16\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 856/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1316352 \n",
            "                       Steps per second: 7016 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.119s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0002\n",
            "                      Mean entropy loss: -4.3115\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0025\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:16\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 857/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1317888 \n",
            "                       Steps per second: 7336 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0161\n",
            "                      Mean entropy loss: -4.1919\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0020\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:16\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 858/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1319424 \n",
            "                       Steps per second: 6877 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.120s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: -4.1824\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:17\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 859/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1320960 \n",
            "                       Steps per second: 7070 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0137\n",
            "                      Mean entropy loss: -4.1074\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0039\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:17\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 860/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1322496 \n",
            "                       Steps per second: 7121 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: -4.1177\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:17\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 861/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1324032 \n",
            "                       Steps per second: 6763 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: -4.1429\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9995\n",
            "                  Episode_Reward/effort: -0.0062\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:17\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 862/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1325568 \n",
            "                       Steps per second: 7223 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0053\n",
            "                      Mean entropy loss: -4.1018\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:17\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 863/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1327104 \n",
            "                       Steps per second: 7067 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0009\n",
            "                      Mean entropy loss: -4.1058\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:18\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 864/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1328640 \n",
            "                       Steps per second: 6656 \n",
            "                        Collection time: 0.112s \n",
            "                          Learning time: 0.119s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0004\n",
            "                      Mean entropy loss: -4.1899\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0037\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:18\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 865/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1330176 \n",
            "                       Steps per second: 6552 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.129s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0002\n",
            "                      Mean entropy loss: -4.1952\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:18\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 866/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1331712 \n",
            "                       Steps per second: 7095 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -4.1628\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:18\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 867/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1333248 \n",
            "                       Steps per second: 6978 \n",
            "                        Collection time: 0.113s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0057\n",
            "                      Mean entropy loss: -4.1799\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:19\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 868/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1334784 \n",
            "                       Steps per second: 7422 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0004\n",
            "                    Mean surrogate loss: 0.0225\n",
            "                      Mean entropy loss: -4.1640\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:19\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 869/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1336320 \n",
            "                       Steps per second: 6980 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: -0.0028\n",
            "                      Mean entropy loss: -4.1595\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:19\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 870/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1337856 \n",
            "                       Steps per second: 7042 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0009\n",
            "                    Mean surrogate loss: 0.0012\n",
            "                      Mean entropy loss: -4.2226\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:19\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 871/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1339392 \n",
            "                       Steps per second: 7311 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0007\n",
            "                    Mean surrogate loss: 0.0250\n",
            "                      Mean entropy loss: -4.2200\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0026\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:19\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 872/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1340928 \n",
            "                       Steps per second: 7473 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0079\n",
            "                      Mean entropy loss: -4.1836\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0026\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:20\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 873/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1342464 \n",
            "                       Steps per second: 7472 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0039\n",
            "                      Mean entropy loss: -4.1452\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9996\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:20\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 874/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1344000 \n",
            "                       Steps per second: 7037 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.122s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0066\n",
            "                      Mean entropy loss: -4.1304\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9996\n",
            "                  Episode_Reward/effort: -0.0056\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:20\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 875/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1345536 \n",
            "                       Steps per second: 7475 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0089\n",
            "                      Mean entropy loss: -4.1442\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:20\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 876/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1347072 \n",
            "                       Steps per second: 7576 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0008\n",
            "                      Mean entropy loss: -4.1767\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:20\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 877/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1348608 \n",
            "                       Steps per second: 7322 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0211\n",
            "                      Mean entropy loss: -4.2234\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:21\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 878/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1350144 \n",
            "                       Steps per second: 7450 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0019\n",
            "                      Mean entropy loss: -4.1982\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:21\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 879/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1351680 \n",
            "                       Steps per second: 6128 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0083\n",
            "                      Mean entropy loss: -4.2265\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0029\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:03:21\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 880/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1353216 \n",
            "                       Steps per second: 5376 \n",
            "                        Collection time: 0.141s \n",
            "                          Learning time: 0.145s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0042\n",
            "                      Mean entropy loss: -4.2335\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:03:21\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 881/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1354752 \n",
            "                       Steps per second: 5557 \n",
            "                        Collection time: 0.135s \n",
            "                          Learning time: 0.141s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0036\n",
            "                      Mean entropy loss: -4.2044\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:03:22\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 882/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1356288 \n",
            "                       Steps per second: 5818 \n",
            "                        Collection time: 0.120s \n",
            "                          Learning time: 0.144s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0014\n",
            "                      Mean entropy loss: -4.2635\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9996\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:03:22\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 883/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1357824 \n",
            "                       Steps per second: 5744 \n",
            "                        Collection time: 0.134s \n",
            "                          Learning time: 0.133s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: -4.2628\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9996\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:22\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 884/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1359360 \n",
            "                       Steps per second: 5804 \n",
            "                        Collection time: 0.125s \n",
            "                          Learning time: 0.139s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0071\n",
            "                      Mean entropy loss: -4.2835\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:03:23\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 885/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1360896 \n",
            "                       Steps per second: 5433 \n",
            "                        Collection time: 0.135s \n",
            "                          Learning time: 0.148s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: -4.2549\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0059\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:03:23\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 886/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1362432 \n",
            "                       Steps per second: 4998 \n",
            "                        Collection time: 0.129s \n",
            "                          Learning time: 0.178s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0003\n",
            "                      Mean entropy loss: -4.3081\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0037\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:03:23\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 887/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1363968 \n",
            "                       Steps per second: 5014 \n",
            "                        Collection time: 0.147s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: -4.3050\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0032\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:03:23\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 888/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1365504 \n",
            "                       Steps per second: 5648 \n",
            "                        Collection time: 0.155s \n",
            "                          Learning time: 0.117s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0172\n",
            "                      Mean entropy loss: -4.2137\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:24\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 889/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1367040 \n",
            "                       Steps per second: 7548 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0021\n",
            "                      Mean entropy loss: -4.1946\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0058\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:24\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 890/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1368576 \n",
            "                       Steps per second: 6779 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.120s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0000\n",
            "                      Mean entropy loss: -4.1560\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:24\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 891/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1370112 \n",
            "                       Steps per second: 7263 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0102\n",
            "                      Mean entropy loss: -4.1783\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:24\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 892/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1371648 \n",
            "                       Steps per second: 7706 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: -4.1419\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:25\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 893/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1373184 \n",
            "                       Steps per second: 7406 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: -4.1152\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:25\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 894/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1374720 \n",
            "                       Steps per second: 7412 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0003\n",
            "                      Mean entropy loss: -4.0433\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0028\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:25\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 895/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1376256 \n",
            "                       Steps per second: 6953 \n",
            "                        Collection time: 0.112s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0031\n",
            "                      Mean entropy loss: -4.0491\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:25\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 896/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1377792 \n",
            "                       Steps per second: 7021 \n",
            "                        Collection time: 0.092s \n",
            "                          Learning time: 0.127s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0090\n",
            "                      Mean entropy loss: -4.0433\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0070\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:25\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 897/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1379328 \n",
            "                       Steps per second: 7364 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: -4.0192\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0070\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:26\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 898/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1380864 \n",
            "                       Steps per second: 7367 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0326\n",
            "                      Mean entropy loss: -3.8598\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:26\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 899/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1382400 \n",
            "                       Steps per second: 6986 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.122s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: -3.8780\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0039\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:26\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 900/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1383936 \n",
            "                       Steps per second: 7241 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0043\n",
            "                      Mean entropy loss: -3.9908\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0046\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:26\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 901/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1385472 \n",
            "                       Steps per second: 6696 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.125s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0016\n",
            "                      Mean entropy loss: -3.9938\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:26\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 902/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1387008 \n",
            "                       Steps per second: 7039 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0166\n",
            "                      Mean entropy loss: -3.9622\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0029\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:27\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 903/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1388544 \n",
            "                       Steps per second: 6916 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0000\n",
            "                      Mean entropy loss: -3.9905\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0024\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:27\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 904/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1390080 \n",
            "                       Steps per second: 6696 \n",
            "                        Collection time: 0.119s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: -3.9865\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9996\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:27\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 905/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1391616 \n",
            "                       Steps per second: 7242 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0026\n",
            "                      Mean entropy loss: -4.0253\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:27\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 906/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1393152 \n",
            "                       Steps per second: 6972 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.115s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0082\n",
            "                      Mean entropy loss: -4.0485\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:28\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 907/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1394688 \n",
            "                       Steps per second: 7311 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0074\n",
            "                      Mean entropy loss: -4.0311\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:28\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 908/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1396224 \n",
            "                       Steps per second: 6500 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.123s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0017\n",
            "                      Mean entropy loss: -4.0493\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:03:28\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 909/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1397760 \n",
            "                       Steps per second: 6756 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0040\n",
            "                      Mean entropy loss: -4.1347\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:28\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 910/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1399296 \n",
            "                       Steps per second: 7451 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0005\n",
            "                    Mean surrogate loss: 0.0089\n",
            "                      Mean entropy loss: -4.1017\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:28\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 911/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1400832 \n",
            "                       Steps per second: 6960 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: 0.0000\n",
            "                      Mean entropy loss: -4.1252\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0029\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:29\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 912/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1402368 \n",
            "                       Steps per second: 7330 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: -4.1535\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:29\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 913/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1403904 \n",
            "                       Steps per second: 6882 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0047\n",
            "                      Mean entropy loss: -4.1946\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:29\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 914/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1405440 \n",
            "                       Steps per second: 6869 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.126s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0012\n",
            "                      Mean entropy loss: -4.1474\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:29\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 915/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1406976 \n",
            "                       Steps per second: 7454 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0107\n",
            "                      Mean entropy loss: -4.0764\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9994\n",
            "                  Episode_Reward/effort: -0.0069\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:30\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 916/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1408512 \n",
            "                       Steps per second: 7344 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0056\n",
            "                      Mean entropy loss: -4.0759\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:30\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 917/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1410048 \n",
            "                       Steps per second: 7145 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.121s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0208\n",
            "                      Mean entropy loss: -4.0900\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0051\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:30\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 918/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1411584 \n",
            "                       Steps per second: 7410 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0081\n",
            "                      Mean entropy loss: -4.0869\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0051\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:30\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 919/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1413120 \n",
            "                       Steps per second: 7537 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0052\n",
            "                      Mean entropy loss: -4.0755\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0051\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:30\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 920/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1414656 \n",
            "                       Steps per second: 7292 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -4.0504\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0030\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:31\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 921/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1416192 \n",
            "                       Steps per second: 7492 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0009\n",
            "                      Mean entropy loss: -4.0602\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:31\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 922/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1417728 \n",
            "                       Steps per second: 6736 \n",
            "                        Collection time: 0.120s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0004\n",
            "                      Mean entropy loss: -4.0416\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:31\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 923/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1419264 \n",
            "                       Steps per second: 7000 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0121\n",
            "                      Mean entropy loss: -4.0226\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:31\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 924/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1420800 \n",
            "                       Steps per second: 7308 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0045\n",
            "                      Mean entropy loss: -3.9690\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:31\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 925/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1422336 \n",
            "                       Steps per second: 7188 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0149\n",
            "                      Mean entropy loss: -3.9086\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0039\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:32\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 926/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1423872 \n",
            "                       Steps per second: 6706 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.126s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0021\n",
            "                      Mean entropy loss: -3.8824\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:32\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 927/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1425408 \n",
            "                       Steps per second: 6903 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.117s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0061\n",
            "                      Mean entropy loss: -3.8184\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:32\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 928/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1426944 \n",
            "                       Steps per second: 7027 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: -3.7953\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:32\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 929/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1428480 \n",
            "                       Steps per second: 7085 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0069\n",
            "                      Mean entropy loss: -3.7981\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:33\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 930/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1430016 \n",
            "                       Steps per second: 7140 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0104\n",
            "                      Mean entropy loss: -3.8188\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:33\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 931/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1431552 \n",
            "                       Steps per second: 6969 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0101\n",
            "                      Mean entropy loss: -3.8325\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:33\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 932/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1433088 \n",
            "                       Steps per second: 5226 \n",
            "                        Collection time: 0.145s \n",
            "                          Learning time: 0.149s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0025\n",
            "                      Mean entropy loss: -3.8516\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:03:33\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 933/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1434624 \n",
            "                       Steps per second: 5748 \n",
            "                        Collection time: 0.127s \n",
            "                          Learning time: 0.140s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0147\n",
            "                      Mean entropy loss: -3.8296\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:34\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 934/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1436160 \n",
            "                       Steps per second: 6008 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.140s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0077\n",
            "                      Mean entropy loss: -3.8679\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:03:34\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 935/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1437696 \n",
            "                       Steps per second: 5700 \n",
            "                        Collection time: 0.122s \n",
            "                          Learning time: 0.147s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0048\n",
            "                      Mean entropy loss: -3.8129\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:34\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 936/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1439232 \n",
            "                       Steps per second: 6182 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.140s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0071\n",
            "                      Mean entropy loss: -3.9307\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:03:34\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 937/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1440768 \n",
            "                       Steps per second: 5895 \n",
            "                        Collection time: 0.122s \n",
            "                          Learning time: 0.138s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0053\n",
            "                      Mean entropy loss: -3.9237\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0032\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:03:35\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 938/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1442304 \n",
            "                       Steps per second: 5634 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.157s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0020\n",
            "                      Mean entropy loss: -3.9987\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0027\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:35\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 939/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1443840 \n",
            "                       Steps per second: 5223 \n",
            "                        Collection time: 0.131s \n",
            "                          Learning time: 0.163s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0068\n",
            "                      Mean entropy loss: -4.0207\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0027\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:03:35\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 940/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1445376 \n",
            "                       Steps per second: 4744 \n",
            "                        Collection time: 0.142s \n",
            "                          Learning time: 0.182s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0132\n",
            "                      Mean entropy loss: -4.0989\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0051\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:03:35\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 941/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1446912 \n",
            "                       Steps per second: 7541 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0010\n",
            "                      Mean entropy loss: -4.0854\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9996\n",
            "                  Episode_Reward/effort: -0.0054\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:36\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 942/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1448448 \n",
            "                       Steps per second: 6822 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.120s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0006\n",
            "                      Mean entropy loss: -4.1669\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:36\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 943/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1449984 \n",
            "                       Steps per second: 7225 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0020\n",
            "                      Mean entropy loss: -4.1250\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:36\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 944/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1451520 \n",
            "                       Steps per second: 6938 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0069\n",
            "                      Mean entropy loss: -4.1245\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:36\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 945/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1453056 \n",
            "                       Steps per second: 7034 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.117s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0249\n",
            "                      Mean entropy loss: -4.0227\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:37\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 946/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1454592 \n",
            "                       Steps per second: 6485 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.121s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0003\n",
            "                      Mean entropy loss: -4.0465\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:03:37\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 947/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1456128 \n",
            "                       Steps per second: 6338 \n",
            "                        Collection time: 0.122s \n",
            "                          Learning time: 0.120s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: -4.0735\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:03:37\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 948/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1457664 \n",
            "                       Steps per second: 7109 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0054\n",
            "                      Mean entropy loss: -4.0769\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:37\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 949/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1459200 \n",
            "                       Steps per second: 7059 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0068\n",
            "                      Mean entropy loss: -4.0494\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:37\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 950/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1460736 \n",
            "                       Steps per second: 6865 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: -4.0524\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0037\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:38\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 951/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1462272 \n",
            "                       Steps per second: 6806 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0033\n",
            "                      Mean entropy loss: -4.1008\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:38\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 952/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1463808 \n",
            "                       Steps per second: 6966 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0027\n",
            "                      Mean entropy loss: -4.1156\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:38\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 953/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1465344 \n",
            "                       Steps per second: 6325 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.146s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0012\n",
            "                      Mean entropy loss: -4.1196\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0058\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:03:38\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 954/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1466880 \n",
            "                       Steps per second: 7142 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: -4.0627\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0046\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:39\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 955/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1468416 \n",
            "                       Steps per second: 7017 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.123s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0020\n",
            "                      Mean entropy loss: -4.1344\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:39\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 956/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1469952 \n",
            "                       Steps per second: 7335 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: -4.0254\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:39\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 957/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1471488 \n",
            "                       Steps per second: 7285 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: -4.1216\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:39\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 958/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1473024 \n",
            "                       Steps per second: 7430 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0055\n",
            "                      Mean entropy loss: -4.0314\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:39\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 959/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1474560 \n",
            "                       Steps per second: 7358 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: -3.9408\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:40\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 960/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1476096 \n",
            "                       Steps per second: 7086 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0002\n",
            "                      Mean entropy loss: -4.0698\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:40\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 961/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1477632 \n",
            "                       Steps per second: 7277 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0001\n",
            "                      Mean entropy loss: -4.0052\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:40\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 962/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1479168 \n",
            "                       Steps per second: 7259 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0067\n",
            "                      Mean entropy loss: -4.0553\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:40\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 963/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1480704 \n",
            "                       Steps per second: 7109 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: 0.0090\n",
            "                      Mean entropy loss: -4.0871\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0037\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:40\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 964/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1482240 \n",
            "                       Steps per second: 6706 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.118s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: -0.0024\n",
            "                      Mean entropy loss: -4.1199\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:41\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 965/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1483776 \n",
            "                       Steps per second: 6978 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0038\n",
            "                      Mean entropy loss: -4.1219\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9995\n",
            "                  Episode_Reward/effort: -0.0061\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:41\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 966/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1485312 \n",
            "                       Steps per second: 7260 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: -4.0942\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:41\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 967/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1486848 \n",
            "                       Steps per second: 7009 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0148\n",
            "                      Mean entropy loss: -4.1148\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0032\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:41\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 968/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1488384 \n",
            "                       Steps per second: 7351 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0101\n",
            "                      Mean entropy loss: -4.1232\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0039\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:42\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 969/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1489920 \n",
            "                       Steps per second: 6403 \n",
            "                        Collection time: 0.129s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0021\n",
            "                      Mean entropy loss: -4.1344\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:03:42\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 970/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1491456 \n",
            "                       Steps per second: 7222 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0112\n",
            "                      Mean entropy loss: -4.1086\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:42\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 971/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1492992 \n",
            "                       Steps per second: 6823 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0015\n",
            "                      Mean entropy loss: -4.1434\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:42\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 972/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1494528 \n",
            "                       Steps per second: 7156 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0046\n",
            "                      Mean entropy loss: -4.1280\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:42\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 973/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1496064 \n",
            "                       Steps per second: 6559 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.126s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0112\n",
            "                      Mean entropy loss: -4.2345\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:03:43\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 974/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1497600 \n",
            "                       Steps per second: 6998 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.122s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0032\n",
            "                      Mean entropy loss: -4.1755\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:43\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 975/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1499136 \n",
            "                       Steps per second: 7383 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0060\n",
            "                      Mean entropy loss: -4.1985\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0027\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:43\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 976/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1500672 \n",
            "                       Steps per second: 7511 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0093\n",
            "                      Mean entropy loss: -4.1567\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0024\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:43\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 977/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1502208 \n",
            "                       Steps per second: 7409 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0014\n",
            "                      Mean entropy loss: -4.2674\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0031\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:44\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 978/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1503744 \n",
            "                       Steps per second: 7104 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: -4.2759\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:44\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 979/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1505280 \n",
            "                       Steps per second: 7269 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0030\n",
            "                      Mean entropy loss: -4.2931\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0090\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:44\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 980/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1506816 \n",
            "                       Steps per second: 7568 \n",
            "                        Collection time: 0.092s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0020\n",
            "                      Mean entropy loss: -4.2662\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0090\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:03:44\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 981/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1508352 \n",
            "                       Steps per second: 7367 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0003\n",
            "                    Mean surrogate loss: 0.0075\n",
            "                      Mean entropy loss: -4.2858\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0078\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:44\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 982/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1509888 \n",
            "                       Steps per second: 7309 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0004\n",
            "                    Mean surrogate loss: 0.0082\n",
            "                      Mean entropy loss: -4.2986\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:45\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 983/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1511424 \n",
            "                       Steps per second: 7090 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0031\n",
            "                      Mean entropy loss: -4.3021\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0025\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:45\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 984/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1512960 \n",
            "                       Steps per second: 5591 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.168s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: -4.2635\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:45\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 985/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1514496 \n",
            "                       Steps per second: 5538 \n",
            "                        Collection time: 0.136s \n",
            "                          Learning time: 0.142s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0030\n",
            "                      Mean entropy loss: -4.3367\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:03:45\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 986/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1516032 \n",
            "                       Steps per second: 5756 \n",
            "                        Collection time: 0.123s \n",
            "                          Learning time: 0.144s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0006\n",
            "                      Mean entropy loss: -4.3070\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:46\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 987/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1517568 \n",
            "                       Steps per second: 5729 \n",
            "                        Collection time: 0.133s \n",
            "                          Learning time: 0.135s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0221\n",
            "                      Mean entropy loss: -4.3445\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:03:46\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 988/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1519104 \n",
            "                       Steps per second: 5875 \n",
            "                        Collection time: 0.126s \n",
            "                          Learning time: 0.136s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0051\n",
            "                      Mean entropy loss: -4.2756\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:03:46\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 989/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1520640 \n",
            "                       Steps per second: 5048 \n",
            "                        Collection time: 0.129s \n",
            "                          Learning time: 0.175s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: -4.3330\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:03:46\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 990/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1522176 \n",
            "                       Steps per second: 5272 \n",
            "                        Collection time: 0.126s \n",
            "                          Learning time: 0.165s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0027\n",
            "                      Mean entropy loss: -4.3046\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:03:47\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 991/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1523712 \n",
            "                       Steps per second: 5001 \n",
            "                        Collection time: 0.152s \n",
            "                          Learning time: 0.155s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0101\n",
            "                      Mean entropy loss: -4.3096\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:03:47\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 992/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1525248 \n",
            "                       Steps per second: 4733 \n",
            "                        Collection time: 0.157s \n",
            "                          Learning time: 0.168s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0001\n",
            "                      Mean entropy loss: -4.2768\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.32s\n",
            "                           Time elapsed: 00:03:47\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 993/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1526784 \n",
            "                       Steps per second: 7073 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0091\n",
            "                      Mean entropy loss: -4.2884\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:48\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 994/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1528320 \n",
            "                       Steps per second: 6261 \n",
            "                        Collection time: 0.124s \n",
            "                          Learning time: 0.121s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0021\n",
            "                      Mean entropy loss: -4.2824\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:03:48\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 995/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1529856 \n",
            "                       Steps per second: 7080 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0131\n",
            "                      Mean entropy loss: -4.2920\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9997\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:48\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 996/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1531392 \n",
            "                       Steps per second: 7435 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: -4.3432\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0017\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:48\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 997/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1532928 \n",
            "                       Steps per second: 7131 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0079\n",
            "                      Mean entropy loss: -4.2810\n",
            "                            Mean reward: 49.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0017\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:48\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 998/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1534464 \n",
            "                       Steps per second: 7082 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.122s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0021\n",
            "                      Mean entropy loss: -4.3305\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9999\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:03:49\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                          Learning iteration 999/1000                           \u001b[0m \n",
            "\n",
            "                            Total steps: 1536000 \n",
            "                       Steps per second: 7375 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -4.3426\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9998\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:03:49\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33m2025-12-05_17-47-56\u001b[0m at: \u001b[34mhttps://wandb.ai/ttktjmt-org/mjlab/runs/cw18z3uw\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251205_174803-cw18z3uw/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# This will take several minutes depending on your training configuration\n",
        "# !uv run train Mjlab-Cartpole --agent.max-iterations 1000 --agent.save-interval 300\n",
        "# !python ./src/mjlab/scripts/train.py Mjlab-Cartpole --help\n",
        "!python /content/mjlab/src/mjlab/scripts/train.py Mjlab-Cartpole --agent.max-iterations 1000 --agent.save-interval 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCaqPznGrx8H"
      },
      "source": [
        "### **üìÅ Locate Training Checkpoints**\n",
        "\n",
        "After training, checkpoints are saved locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPnmHYu8r0uY",
        "outputId": "0aca4fcc-91da-449d-ad7d-45c872ee7d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Latest training run: 2025-12-05_17-47-56\n",
            "\n",
            "Found 5 checkpoints:\n",
            "  ‚Ä¢ model_0.pt (0.99 MB)\n",
            "  ‚Ä¢ model_300.pt (1.00 MB)\n",
            "  ‚Ä¢ model_600.pt (1.00 MB)\n",
            "  ‚Ä¢ model_900.pt (1.00 MB)\n",
            "  ‚Ä¢ model_999.pt (1.00 MB)\n",
            "\n",
            "üíæ Last checkpoint: logs/rsl_rl/cartpole/2025-12-05_17-47-56/model_999.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Find the most recent training run\n",
        "log_dir = Path(\"logs/rsl_rl/cartpole\")\n",
        "if log_dir.exists():\n",
        "    runs = sorted(log_dir.glob(\"*\"), key=os.path.getmtime, reverse=True)\n",
        "    if runs:\n",
        "        latest_run = runs[0]\n",
        "        print(f\"‚úì Latest training run: {latest_run.name}\\n\")\n",
        "\n",
        "        # List checkpoints\n",
        "        checkpoints = sorted(latest_run.glob(\"model_*.pt\"))\n",
        "        if checkpoints:\n",
        "            print(f\"Found {len(checkpoints)} checkpoints:\")\n",
        "            for ckpt in checkpoints[-5:]:  # Show last 5\n",
        "                size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
        "                print(f\"  ‚Ä¢ {ckpt.name} ({size_mb:.2f} MB)\")\n",
        "\n",
        "            # Store the last checkpoint path\n",
        "            last_checkpoint = str(checkpoints[-1])\n",
        "            print(f\"\\nüíæ Last checkpoint: {last_checkpoint}\")\n",
        "        else:\n",
        "            print(\"‚ö† No checkpoints found yet\")\n",
        "    else:\n",
        "        print(\"‚ö† No training runs found\")\n",
        "else:\n",
        "    print(\"‚ö† Log directory not found. Have you run training yet?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWFS9Pw7r2uH"
      },
      "source": [
        "---\n",
        "\n",
        "## **üéÆ Step 4: Visualize the Trained Policy**\n",
        "\n",
        "Let's see the trained policy in action!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78PgHtpfr5sb"
      },
      "source": [
        "### **üåê Launch the Viser Viewer API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9tGiFyBr2bW",
        "outputId": "3fbd3ba7-644c-407c-de2f-60e890ac80cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: Loading checkpoint: model_999.pt\n",
            "Warp 1.10.1 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.10.1\n",
            "Module mujoco_warp._src.smooth 9ca7ec0 load on device 'cuda:0' took 4.05 ms  (cached)\n",
            "Module mujoco_warp._src.collision_driver e72006d load on device 'cuda:0' took 0.33 ms  (cached)\n",
            "Module _nxn_broadphase__locals__kernel_1799b5b8 1799b5b load on device 'cuda:0' took 0.32 ms  (cached)\n",
            "Module mujoco_warp._src.collision_primitive._create_narrowphase_kernel f53bec7 load on device 'cuda:0' took 2.62 ms  (cached)\n",
            "Module mujoco_warp._src.constraint fa42ba8 load on device 'cuda:0' took 1.37 ms  (cached)\n",
            "Module _actuator_velocity__locals__actuator_velocity_7933d235 876a329 load on device 'cuda:0' took 0.53 ms  (cached)\n",
            "Module mujoco_warp._src.passive fc4f8e1 load on device 'cuda:0' took 0.77 ms  (cached)\n",
            "Module mujoco_warp._src.forward a88f545 load on device 'cuda:0' took 1.91 ms  (cached)\n",
            "Module mujoco_warp._src.support 769a44d load on device 'cuda:0' took 0.41 ms  (cached)\n",
            "Module _tile_cholesky_factorize_solve__locals__cholesky_factorize_solve_43a10bd2 d7e04e1 load on device 'cuda:0' took 87.59 ms  (cached)\n",
            "Module mujoco_warp._src.solver 1699532 load on device 'cuda:0' took 0.68 ms  (cached)\n",
            "Module mul_m_dense__locals___mul_m_dense_34acc5d2 6f70d43 load on device 'cuda:0' took 28.85 ms  (cached)\n",
            "Module update_constraint_gauss_cost__locals__kernel_fd0aa713 fd0aa71 load on device 'cuda:0' took 0.36 ms  (cached)\n",
            "Module update_gradient_JTDAJ_dense_tiled__locals__kernel_fcb58f0e 913eb00 load on device 'cuda:0' took 48.63 ms  (cached)\n",
            "Module update_gradient_cholesky__locals__kernel_b2b7bfef 2ca8f88 load on device 'cuda:0' took 55.18 ms  (cached)\n",
            "Module linesearch_jv_fused__locals__kernel_90eb52be 90eb52b load on device 'cuda:0' took 0.57 ms  (cached)\n",
            "Module mujoco_warp._src.derivative fda8455 load on device 'cuda:0' took 0.43 ms  (cached)\n",
            "\n",
            "+---------------------------------+\n",
            "|         Base Environment        |\n",
            "+------------------------+--------+\n",
            "| Property               | Value  |\n",
            "+------------------------+--------+\n",
            "| Number of environments | 4      |\n",
            "| Environment device     | cuda:0 |\n",
            "| Environment seed       | None   |\n",
            "| Physics step-size      | 0.02   |\n",
            "| Environment step-size  | 0.02   |\n",
            "+------------------------+--------+\n",
            "\n",
            "[INFO] <EventManager> contains 1 active terms.\n",
            "+-------------------------------------+\n",
            "| Active Event Terms in Mode: 'reset' |\n",
            "+--------+----------------------------+\n",
            "| Index  | Name                       |\n",
            "+--------+----------------------------+\n",
            "|   0    | reset_robot_joints         |\n",
            "+--------+----------------------------+\n",
            "\n",
            "[INFO] <NullCommandManager> (inactive)\n",
            "[INFO] <ActionManager> contains 1 active terms.\n",
            "+--------------------------------+\n",
            "| Active Action Terms (shape: 1) |\n",
            "+-------+-----------+------------+\n",
            "| Index | Name      |  Dimension |\n",
            "+-------+-----------+------------+\n",
            "|   0   | joint_pos |          1 |\n",
            "+-------+-----------+------------+\n",
            "\n",
            "[INFO] <ObservationManager> contains 2 groups.\n",
            "+-----------------------------------------------------------+\n",
            "| Active Observation Terms in Group: 'policy' (shape: (4,)) |\n",
            "+----------------+------------------------+-----------------+\n",
            "|     Index      | Name                   |      Shape      |\n",
            "+----------------+------------------------+-----------------+\n",
            "|       0        | angle                  |       (1,)      |\n",
            "|       1        | ang_vel                |       (1,)      |\n",
            "|       2        | cart_pos               |       (1,)      |\n",
            "|       3        | cart_vel               |       (1,)      |\n",
            "+----------------+------------------------+-----------------+\n",
            "+-----------------------------------------------------------+\n",
            "| Active Observation Terms in Group: 'critic' (shape: (4,)) |\n",
            "+----------------+------------------------+-----------------+\n",
            "|     Index      | Name                   |      Shape      |\n",
            "+----------------+------------------------+-----------------+\n",
            "|       0        | angle                  |       (1,)      |\n",
            "|       1        | ang_vel                |       (1,)      |\n",
            "|       2        | cart_pos               |       (1,)      |\n",
            "|       3        | cart_vel               |       (1,)      |\n",
            "+----------------+------------------------+-----------------+\n",
            "\n",
            "[INFO] <TerminationManager> contains 2 active terms.\n",
            "+----------------------------+\n",
            "|  Active Termination Terms  |\n",
            "+-------+---------+----------+\n",
            "| Index | Name    | Time Out |\n",
            "+-------+---------+----------+\n",
            "|   0   | timeout |   True   |\n",
            "|   1   | tipped  |  False   |\n",
            "+-------+---------+----------+\n",
            "\n",
            "[INFO] <RewardManager> contains 2 active terms.\n",
            "+--------------------------+\n",
            "|   Active Reward Terms    |\n",
            "+-------+---------+--------+\n",
            "| Index | Name    | Weight |\n",
            "+-------+---------+--------+\n",
            "|   0   | upright |    5.0 |\n",
            "|   1   | effort  |    1.0 |\n",
            "+-------+---------+--------+\n",
            "\n",
            "[INFO] <NullCurriculumManager> (inactive)\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ('policy',)\n",
            "\t critic :  ('critic',)\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ viser (listening *:8081) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n",
            "\n",
            "====================================================\n",
            "‚úÖ Server is running! Execute the next cell to view.\n",
            "====================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "process = subprocess.Popen(\n",
        "  [\n",
        "    \"python\",\n",
        "    \"/content/mjlab/src/mjlab/scripts/play.py\",\n",
        "    \"Mjlab-Cartpole\",\n",
        "    \"--checkpoint_file\",\n",
        "    last_checkpoint,\n",
        "    \"--num_envs\",\n",
        "    \"4\",\n",
        "  ],\n",
        "  stdout=subprocess.PIPE,\n",
        "  stderr=subprocess.STDOUT,\n",
        "  universal_newlines=True,\n",
        "  bufsize=1,\n",
        ")\n",
        "\n",
        "for line in process.stdout:\n",
        "  print(line, end=\"\")\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  if \"serving\" in line.lower() or \"running on\" in line.lower() or \"8081\" in line:\n",
        "    print(\"\\n\" + \"=\" * 52)\n",
        "    print(\"‚úÖ Server is running! Execute the next cell to view.\")\n",
        "    print(\"=\" * 52)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "ll89QnuSuUxx",
        "outputId": "9ecea04c-c4c9-406f-cdd9-af91d241d557"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8081, \"/\", \"100%\", 700, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import output\n",
        "\n",
        "output.serve_kernel_port_as_iframe(\n",
        "    port=8081,\n",
        "    height=700\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN7GIEjVr9vw"
      },
      "source": [
        "### **üìπ Generate Video Recording**\n",
        "\n",
        "Record a video of the trained policy for visualization as `.viser` format."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}