{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttktjmt/mjlab/blob/main/notebooks/create_new_task.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO76KS1i-MwA"
      },
      "source": [
        "# **ü§ñ CartPole Tutorial with MJLab**\n",
        "\n",
        "This notebook demonstrates how to create a custom reinforcement learning task using MJLab. We'll build a CartPole environment from scratch, including:\n",
        "\n",
        "1. **Robot Definition** - Define the CartPole model in MuJoCo XML\n",
        "2. **Task Configuration** - Set up observations, actions, rewards, and terminations\n",
        "3. **Training** - Train a policy using PPO\n",
        "4. **Evaluation** - Visualize/Record the trained policy\n",
        "\n",
        "> **Note**: This tutorial is created based on the official MJLab documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ywZTgfR3C_w"
      },
      "source": [
        "## **üì¶ Setup and Installation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dtLMJHzy3Nee",
        "outputId": "2aa95c30-88c4-4b19-f5ab-c37f9dde1836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mjlab\n",
            "‚úì Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# Clone the mjlab repository\n",
        "!if [ ! -d \"mjlab\" ]; then git clone -q https://github.com/mujocolab/mjlab.git; fi\n",
        "%cd /content/mjlab\n",
        "\n",
        "# Install mjlab in editable mode\n",
        "!uv pip install --system -e . -q\n",
        "\n",
        "print(\"‚úì Installation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSf2943z3b0s"
      },
      "source": [
        "### **üîë WandB Setup (Optional)**\n",
        "\n",
        "Configure Weights & Biases for experiment tracking. Add your WandB API key to Colab Secrets:\n",
        "- `WANDB_API_KEY`: from [wandb.ai/authorize](https://wandb.ai/authorize)\n",
        "- `WANDB_ENTITY`: your wandb entity name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KC9ywCnm3dGg",
        "outputId": "3faf8685-fbd5-4bbf-84e4-c83b289d9ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì WandB configured successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    # Set this to disable wandb logger\n",
        "    # os.environ['WANDB_MODE'] = 'disabled'\n",
        "\n",
        "    # Set this to use wandb logger\n",
        "    os.environ['WANDB_API_KEY'] = userdata.get('WANDB_API_KEY')\n",
        "    os.environ['WANDB_ENTITY'] = userdata.get('WANDB_ENTITY')\n",
        "\n",
        "    print(\"‚úì WandB configured successfully!\")\n",
        "except (AttributeError, KeyError):\n",
        "    print(\"‚ö† WandB secrets not found. Training will proceed without WandB logging.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mispfmy73lmq"
      },
      "source": [
        "---\n",
        "\n",
        "## **ü§ñ Step 1: Define the Robot**\n",
        "\n",
        "We'll create a simple CartPole robot with:\n",
        "- A sliding cart (1 DOF)\n",
        "- A hinged pole (1 DOF)\n",
        "- A velocity actuator to control the cart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FvJYPWD3scd"
      },
      "source": [
        "### **üìÅ Structure Directories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OP-yET-R3ofN",
        "outputId": "9f0c5792-2f70-4ff9-db0b-1f81812163f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Directory structure created\n"
          ]
        }
      ],
      "source": [
        "# Create the cartpole robot directory structure\n",
        "!mkdir -p /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/\n",
        "!mkdir -p /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/xmls\n",
        "\n",
        "print(\"‚úì Directory structure created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRyN1Pok3u25"
      },
      "source": [
        "### **üìù Create MuJoCo XML Model**\n",
        "\n",
        "This XML defines the CartPole physics:\n",
        "- **Ground plane** for visualization\n",
        "- **Cart body** with a sliding joint (¬±2m range)\n",
        "- **Pole body** with a hinge joint (¬±90¬∞ range)\n",
        "- **Velocity actuator** for cart control"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWGyFX5V3yWc",
        "outputId": "86757235-634c-480f-f995-ffcb080bc79a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/xmls/cartpole.xml\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/xmls/cartpole.xml\n",
        "<mujoco model=\"cartpole\">\n",
        "  <compiler angle=\"degree\" coordinate=\"local\" inertiafromgeom=\"true\"/>\n",
        "  <worldbody>\n",
        "    <geom name=\"ground\" type=\"plane\" pos=\"0 0 0\" size=\"5 5 0.1\" rgba=\"0.8 0.9 0.8 1\"/>\n",
        "    <body name=\"cart\" pos=\"0 0 0.1\">\n",
        "      <geom type=\"box\" size=\"0.2 0.1 0.1\" rgba=\"0.2 0.2 0.8 1\" mass=\"1.0\"/>\n",
        "      <joint name=\"slide\" type=\"slide\" axis=\"1 0 0\" limited=\"true\" range=\"-2 2\"/>\n",
        "      <body name=\"pole\" pos=\"0 0 0.1\">\n",
        "        <geom type=\"capsule\" size=\"0.05 0.5\" fromto=\"0 0 0 0 0 1\" rgba=\"0.8 0.2 0.2 1\" mass=\"2.0\"/>\n",
        "        <joint name=\"hinge\" type=\"hinge\" axis=\"0 1 0\" range=\"-90 90\"/>\n",
        "      </body>\n",
        "    </body>\n",
        "  </worldbody>\n",
        "  <actuator>\n",
        "    <velocity name=\"slide_velocity\" joint=\"slide\" ctrlrange=\"-20 20\" kv=\"20\"/>\n",
        "  </actuator>\n",
        "</mujoco>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpYCG9jI31dZ"
      },
      "source": [
        "### **‚öôÔ∏è Create Robot Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDhiyDTn4AVa",
        "outputId": "3d917f1c-f1ee-4587-a0ed-3c0ba5983d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/cartpole_constants.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/cartpole_constants.py\n",
        "from pathlib import Path\n",
        "import mujoco\n",
        "\n",
        "from mjlab import MJLAB_SRC_PATH\n",
        "from mjlab.entity import Entity, EntityCfg, EntityArticulationInfoCfg\n",
        "from mjlab.actuator import XmlVelocityActuatorCfg\n",
        "\n",
        "CARTPOLE_XML: Path = (\n",
        "  MJLAB_SRC_PATH / \"asset_zoo\" / \"robots\" / \"cartpole\" / \"xmls\" / \"cartpole.xml\"\n",
        ")\n",
        "assert CARTPOLE_XML.exists(), f\"XML not found: {CARTPOLE_XML}\"\n",
        "\n",
        "def get_spec() -> mujoco.MjSpec:\n",
        "  return mujoco.MjSpec.from_file(str(CARTPOLE_XML))\n",
        "\n",
        "def get_cartpole_robot_cfg() -> EntityCfg:\n",
        "  \"\"\"Get a fresh CartPole robot configuration instance.\"\"\"\n",
        "  actuators = (\n",
        "    XmlVelocityActuatorCfg(\n",
        "      joint_names_expr=(\"slide\",),\n",
        "    ),\n",
        "  )\n",
        "  articulation = EntityArticulationInfoCfg(actuators=actuators)\n",
        "  return EntityCfg(\n",
        "    spec_fn=get_spec,\n",
        "    articulation=articulation\n",
        "  )\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#   import mujoco.viewer as viewer\n",
        "#   robot = Entity(get_cartpole_robot_cfg())\n",
        "#   viewer.launch(robot.spec.compile())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WSaDod04FwN",
        "outputId": "46846a68-1780-4269-e399-0560c268a286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Create __init__.py for the cartpole robot package\n",
        "%%writefile /content/mjlab/src/mjlab/asset_zoo/robots/cartpole/__init__.py\n",
        "# Empty __init__.py to mark the directory as a Python package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1tiBPfp_oVP",
        "outputId": "3f34db90-d44a-4b07-b19c-0ff7023d2fb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Added /content/mjlab/src to Python path\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Append src dir to python path\n",
        "mjlab_src = '/content/mjlab/src'\n",
        "if mjlab_src not in sys.path:\n",
        "    sys.path.insert(0, mjlab_src)\n",
        "    print(f\"‚úì Added {mjlab_src} to Python path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToWF84qC4Hfg"
      },
      "source": [
        "### **‚úÖ Verify Robot Setup**\n",
        "\n",
        "Let's test that the robot can be loaded correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tVsvqzQ4J9h",
        "outputId": "d3154a17-467d-4ceb-a144-863df42616cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì CartPole robot loaded successfully!\n",
            "  ‚Ä¢ Degrees of Freedom (DOF): 2\n",
            "  ‚Ä¢ Number of Actuators: 1\n",
            "  ‚Ä¢ Bodies: 4\n",
            "  ‚Ä¢ Joints: 2\n"
          ]
        }
      ],
      "source": [
        "from mjlab.entity import Entity\n",
        "from mjlab.asset_zoo.robots.cartpole.cartpole_constants import get_cartpole_robot_cfg\n",
        "\n",
        "# Load the robot\n",
        "robot = Entity(get_cartpole_robot_cfg())\n",
        "model = robot.spec.compile()\n",
        "\n",
        "# Display robot information\n",
        "print(\"‚úì CartPole robot loaded successfully!\")\n",
        "print(f\"  ‚Ä¢ Degrees of Freedom (DOF): {model.nv}\")\n",
        "print(f\"  ‚Ä¢ Number of Actuators: {model.nu}\")\n",
        "print(f\"  ‚Ä¢ Bodies: {model.nbody}\")\n",
        "print(f\"  ‚Ä¢ Joints: {model.njnt}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2_9dixlHON1"
      },
      "source": [
        "### **üìã Register the Robot**\n",
        "\n",
        "Add the CartPole robot to the asset zoo registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qDIF__lHPcb",
        "outputId": "4421646c-482f-4c9d-9d1d-7ac1429a2575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì CartPole robot registered in asset zoo\n"
          ]
        }
      ],
      "source": [
        "# Add CartPole import to robots __init__.py\n",
        "with open('/content/mjlab/src/mjlab/asset_zoo/robots/__init__.py', 'a') as f:\n",
        "    f.write('\\n# CartPole robot\\n')\n",
        "    f.write('from mjlab.asset_zoo.robots.cartpole.cartpole_constants import ')\n",
        "    f.write('get_cartpole_robot_cfg as get_cartpole_robot_cfg\\n')\n",
        "\n",
        "print(\"‚úì CartPole robot registered in asset zoo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lVD_L6PHWNm"
      },
      "source": [
        "---\n",
        "\n",
        "## **üéØ Step 2: Define the Task (MDP)**\n",
        "\n",
        "Now we'll define the Markov Decision Process:\n",
        "- **Observations**: pole angle, angular velocity, cart position, cart velocity\n",
        "- **Actions**: cart velocity commands\n",
        "- **Rewards**: upright reward + effort penalty\n",
        "- **Terminations**: pole tips over or timeout\n",
        "- **Events**: random pushes for robustness"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQxe4TBrHb-I"
      },
      "source": [
        "### **üìÅ Create Task Directory**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWBqdkziHc2G",
        "outputId": "5ae86a63-5f95-4ae5-eea2-a47eef124a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Task directory created\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p /content/mjlab/src/mjlab/tasks/cartpole\n",
        "\n",
        "print(\"‚úì Task directory created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJfjPpm0Hhj1"
      },
      "source": [
        "### **üìù Create Environment Configuration**\n",
        "\n",
        "This file contains the MDP (Markov Decision Process) components:\n",
        "1. **Scene Config**: 64 parallel environments\n",
        "2. **Actions**: Joint velocity control with 20.0 scale\n",
        "3. **Observations**: Normalized state variables\n",
        "4. **Rewards**: Upright reward (5.0) + effort penalty (-0.01)\n",
        "5. **Events**: Joint resets + random pushes\n",
        "6. **Terminations**: Pole tipped (>30¬∞) or timeout (10s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "javx9XDIHkFI",
        "outputId": "2d45d039-8f1e-4936-9c6b-9ee201091a72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/tasks/cartpole/env_cfg.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/tasks/cartpole/env_cfg.py\n",
        "\"\"\"CartPole task environment configuration.\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "\n",
        "from mjlab.envs import ManagerBasedRlEnvCfg\n",
        "from mjlab.envs.mdp.actions import JointVelocityActionCfg\n",
        "from mjlab.managers.manager_term_config import (\n",
        "  ObservationGroupCfg,\n",
        "  ObservationTermCfg,\n",
        "  RewardTermCfg,\n",
        "  TerminationTermCfg,\n",
        "  EventTermCfg,\n",
        ")\n",
        "from mjlab.managers.scene_entity_config import SceneEntityCfg\n",
        "from mjlab.scene import SceneCfg\n",
        "from mjlab.sim import MujocoCfg, SimulationCfg\n",
        "from mjlab.viewer import ViewerConfig\n",
        "from mjlab.asset_zoo.robots.cartpole.cartpole_constants import get_cartpole_robot_cfg\n",
        "from mjlab.envs import mdp\n",
        "\n",
        "\n",
        "def cartpole_env_cfg(play: bool = False) -> ManagerBasedRlEnvCfg:\n",
        "  \"\"\"Create CartPole environment configuration.\n",
        "\n",
        "  Args:\n",
        "    play: If True, disables corruption and extends episode length for evaluation.\n",
        "  \"\"\"\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Scene Configuration\n",
        "  # ==============================================================================\n",
        "\n",
        "  scene_cfg = SceneCfg(\n",
        "    num_envs=64 if not play else 16,  # Fewer envs for play mode\n",
        "    extent=1.0,   # Spacing between environments\n",
        "    entities={\"robot\": get_cartpole_robot_cfg()},\n",
        "  )\n",
        "\n",
        "  viewer_cfg = ViewerConfig(\n",
        "    origin_type=ViewerConfig.OriginType.ASSET_BODY,\n",
        "    asset_name=\"robot\",\n",
        "    body_name=\"pole\",\n",
        "    distance=3.0,\n",
        "    elevation=10.0,\n",
        "    azimuth=90.0,\n",
        "  )\n",
        "\n",
        "  sim_cfg = SimulationCfg(\n",
        "    mujoco=MujocoCfg(\n",
        "      timestep=0.02,  # 50 Hz control\n",
        "      iterations=1,\n",
        "    ),\n",
        "  )\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Actions\n",
        "  # ==============================================================================\n",
        "\n",
        "  actions = {\n",
        "    \"joint_pos\": JointVelocityActionCfg(\n",
        "      asset_name=\"robot\",\n",
        "      actuator_names=(\".*\",),\n",
        "      scale=20.0,\n",
        "      use_default_offset=False,\n",
        "    ),\n",
        "  }\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Observations\n",
        "  # ==============================================================================\n",
        "\n",
        "  policy_terms = {\n",
        "    \"angle\": ObservationTermCfg(\n",
        "      func=lambda env: env.sim.data.qpos[:, 1:2] / math.pi\n",
        "    ),\n",
        "    \"ang_vel\": ObservationTermCfg(\n",
        "      func=lambda env: env.sim.data.qvel[:, 1:2] / 5.0\n",
        "    ),\n",
        "    \"cart_pos\": ObservationTermCfg(\n",
        "      func=lambda env: env.sim.data.qpos[:, 0:1] / 2.0\n",
        "    ),\n",
        "    \"cart_vel\": ObservationTermCfg(\n",
        "      func=lambda env: env.sim.data.qvel[:, 0:1] / 20.0\n",
        "    ),\n",
        "  }\n",
        "\n",
        "  observations = {\n",
        "    \"policy\": ObservationGroupCfg(\n",
        "      terms=policy_terms,\n",
        "      concatenate_terms=True,\n",
        "      enable_corruption=not play,  # Disable corruption in play mode\n",
        "    ),\n",
        "    \"critic\": ObservationGroupCfg(\n",
        "      terms=policy_terms,  # Critic uses same observations\n",
        "      concatenate_terms=True,\n",
        "      enable_corruption=False,\n",
        "    ),\n",
        "  }\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Rewards\n",
        "  # ==============================================================================\n",
        "\n",
        "  def compute_upright_reward(env):\n",
        "    \"\"\"Reward for keeping pole upright (cosine of angle).\"\"\"\n",
        "    return env.sim.data.qpos[:, 1].cos()\n",
        "\n",
        "  def compute_effort_penalty(env):\n",
        "    \"\"\"Penalty for control effort.\"\"\"\n",
        "    return -0.01 * (env.sim.data.ctrl[:, 0] ** 2)\n",
        "\n",
        "  rewards = {\n",
        "    \"upright\": RewardTermCfg(func=compute_upright_reward, weight=5.0),\n",
        "    \"effort\": RewardTermCfg(func=compute_effort_penalty, weight=1.0),\n",
        "  }\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Events\n",
        "  # ==============================================================================\n",
        "\n",
        "  def random_push_cart(env, env_ids, force_range=(-5, 5)):\n",
        "    \"\"\"Apply random force to cart for robustness training.\"\"\"\n",
        "    n = len(env_ids)\n",
        "    random_forces = (\n",
        "      torch.rand(n, device=env.device) *\n",
        "      (force_range[1] - force_range[0]) +\n",
        "      force_range[0]\n",
        "    )\n",
        "    env.sim.data.qfrc_applied[env_ids, 0] = random_forces\n",
        "\n",
        "  events = {\n",
        "    \"reset_robot_joints\": EventTermCfg(\n",
        "      func=mdp.reset_joints_by_offset,\n",
        "      mode=\"reset\",\n",
        "      params={\n",
        "        \"asset_cfg\": SceneEntityCfg(\"robot\"),\n",
        "        \"position_range\": (-0.1, 0.1),\n",
        "        \"velocity_range\": (-0.1, 0.1),\n",
        "      },\n",
        "    ),\n",
        "  }\n",
        "\n",
        "  # Add random pushes only in training mode\n",
        "  if not play:\n",
        "    events[\"random_push\"] = EventTermCfg(\n",
        "      func=random_push_cart,\n",
        "      mode=\"interval\",\n",
        "      interval_range_s=(1.0, 2.0),\n",
        "      params={\"force_range\": (-20.0, 20.0)},\n",
        "    )\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Terminations\n",
        "  # ==============================================================================\n",
        "\n",
        "  def check_pole_tipped(env):\n",
        "    \"\"\"Check if pole has tipped beyond 30 degrees.\"\"\"\n",
        "    return env.sim.data.qpos[:, 1].abs() > math.radians(30)\n",
        "\n",
        "  terminations = {\n",
        "    \"timeout\": TerminationTermCfg(func=mdp.time_out, time_out=True),\n",
        "    \"tipped\": TerminationTermCfg(func=check_pole_tipped, time_out=False),\n",
        "  }\n",
        "\n",
        "  # ==============================================================================\n",
        "  # Environment Configuration\n",
        "  # ==============================================================================\n",
        "\n",
        "  return ManagerBasedRlEnvCfg(\n",
        "    scene=scene_cfg,\n",
        "    observations=observations,\n",
        "    actions=actions,\n",
        "    rewards=rewards,\n",
        "    events=events,\n",
        "    terminations=terminations,\n",
        "    sim=sim_cfg,\n",
        "    viewer=viewer_cfg,\n",
        "    decimation=1,           # No action repeat\n",
        "    episode_length_s=int(1e9) if play else 10.0,  # Infinite for play, 10s for training\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC5maMjzSj_X"
      },
      "source": [
        "### **‚öôÔ∏è Create RL Configuration**\n",
        "\n",
        "This file defines the PPO (Proximal Policy Optimization) training parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C81zZm6mSj_X",
        "outputId": "3a2aad95-14bf-4ef1-b8c8-d19c5ea862e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/tasks/cartpole/rl_cfg.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/tasks/cartpole/rl_cfg.py\n",
        "\"\"\"RL configuration for CartPole task.\"\"\"\n",
        "\n",
        "from mjlab.rl.config import (\n",
        "  RslRlOnPolicyRunnerCfg,\n",
        "  RslRlPpoActorCriticCfg,\n",
        "  RslRlPpoAlgorithmCfg,\n",
        ")\n",
        "\n",
        "\n",
        "def cartpole_ppo_runner_cfg() -> RslRlOnPolicyRunnerCfg:\n",
        "  \"\"\"Create RL runner configuration for CartPole task.\"\"\"\n",
        "  return RslRlOnPolicyRunnerCfg(\n",
        "    policy=RslRlPpoActorCriticCfg(\n",
        "      init_noise_std=1.0,\n",
        "      actor_obs_normalization=True,\n",
        "      critic_obs_normalization=True,\n",
        "      actor_hidden_dims=(256, 128, 64),  # Smaller network for simpler task\n",
        "      critic_hidden_dims=(256, 128, 64),\n",
        "      activation=\"elu\",\n",
        "    ),\n",
        "    algorithm=RslRlPpoAlgorithmCfg(\n",
        "      value_loss_coef=1.0,\n",
        "      use_clipped_value_loss=True,\n",
        "      clip_param=0.2,\n",
        "      entropy_coef=0.01,\n",
        "      num_learning_epochs=5,\n",
        "      num_mini_batches=4,\n",
        "      learning_rate=1.0e-3,\n",
        "      schedule=\"adaptive\",\n",
        "      gamma=0.99,\n",
        "      lam=0.95,\n",
        "      desired_kl=0.01,\n",
        "      max_grad_norm=1.0,\n",
        "    ),\n",
        "    experiment_name=\"cartpole\",\n",
        "    save_interval=50,\n",
        "    num_steps_per_env=24,\n",
        "    max_iterations=5_000,  # Fewer iterations for simpler task\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc8-AHGcHt78"
      },
      "source": [
        "### **üìã Register the Task Environment**\n",
        "\n",
        "Register the CartPole task with mjlab registry."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YitUGUBRHxD4",
        "outputId": "82d72c52-10c4-4af5-c926-45ebb30e6e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/mjlab/src/mjlab/tasks/cartpole/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile /content/mjlab/src/mjlab/tasks/cartpole/__init__.py\n",
        "\"\"\"CartPole task registration.\"\"\"\n",
        "\n",
        "from mjlab.tasks.registry import register_mjlab_task\n",
        "from mjlab.tasks.velocity.rl import VelocityOnPolicyRunner\n",
        "\n",
        "from .env_cfg import cartpole_env_cfg\n",
        "from .rl_cfg import cartpole_ppo_runner_cfg\n",
        "\n",
        "register_mjlab_task(\n",
        "  task_id=\"Mjlab-Cartpole\",\n",
        "  env_cfg=cartpole_env_cfg(),\n",
        "  play_env_cfg=cartpole_env_cfg(play=True),\n",
        "  rl_cfg=cartpole_ppo_runner_cfg(),\n",
        "  runner_cls=VelocityOnPolicyRunner,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7wqLZR1rnGn"
      },
      "source": [
        "---\n",
        "\n",
        "## **üöÄ Step 3: Train the Agent**\n",
        "\n",
        "Now let's train a PPO policy to balance the CartPole!\n",
        "\n",
        "**Training Configuration:**\n",
        "- Algorithm: PPO (Proximal Policy Optimization)\n",
        "- Parallel Environments: 64\n",
        "- Episode Length: 10 seconds (500 steps @ 50Hz)\n",
        "- Total Steps: ~5-10 million (adjust as needed)\n",
        "\n",
        "**‚ö†Ô∏è You may need to create a project named \"mjlab\" on wandb UI manually, since google colab doesn't have permission to create a new project.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hht_hF4trqP2",
        "outputId": "7b0098a1-0a24-49dd-ab9e-fd4d498175f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training with: device=cuda:0, seed=42, rank=0\n",
            "[INFO] Logging experiment in directory: logs/rsl_rl/cartpole/2025-12-05_18-11-25\n",
            "\u001b[92mSetting seed: 42\u001b[0m\n",
            "Warp 1.10.1 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.10.1\n",
            "Module mujoco_warp._src.smooth 9ca7ec0 load on device 'cuda:0' took 7674.39 ms  (compiled)\n",
            "Module mujoco_warp._src.collision_driver e72006d load on device 'cuda:0' took 360.65 ms  (compiled)\n",
            "Module _nxn_broadphase__locals__kernel_1799b5b8 1799b5b load on device 'cuda:0' took 780.10 ms  (compiled)\n",
            "Module mujoco_warp._src.collision_primitive._create_narrowphase_kernel f53bec7 load on device 'cuda:0' took 1221.75 ms  (compiled)\n",
            "Module mujoco_warp._src.constraint fa42ba8 load on device 'cuda:0' took 3547.91 ms  (compiled)\n",
            "Module _actuator_velocity__locals__actuator_velocity_7933d235 876a329 load on device 'cuda:0' took 426.08 ms  (compiled)\n",
            "Module mujoco_warp._src.passive fc4f8e1 load on device 'cuda:0' took 1781.71 ms  (compiled)\n",
            "Module mujoco_warp._src.forward a88f545 load on device 'cuda:0' took 2147.22 ms  (compiled)\n",
            "Module mujoco_warp._src.support 769a44d load on device 'cuda:0' took 775.52 ms  (compiled)\n",
            "Module _tile_cholesky_factorize_solve__locals__cholesky_factorize_solve_43a10bd2 d7e04e1 load on device 'cuda:0' took 3974.12 ms  (compiled)\n",
            "Module mujoco_warp._src.solver 1699532 load on device 'cuda:0' took 2973.57 ms  (compiled)\n",
            "Module mul_m_dense__locals___mul_m_dense_34acc5d2 6f70d43 load on device 'cuda:0' took 7407.05 ms  (compiled)\n",
            "Module update_constraint_gauss_cost__locals__kernel_fd0aa713 fd0aa71 load on device 'cuda:0' took 488.03 ms  (compiled)\n",
            "Module update_gradient_JTDAJ_dense_tiled__locals__kernel_fcb58f0e 913eb00 load on device 'cuda:0' took 6879.25 ms  (compiled)\n",
            "Module update_gradient_cholesky__locals__kernel_b2b7bfef 2ca8f88 load on device 'cuda:0' took 4412.22 ms  (compiled)\n",
            "Module linesearch_jv_fused__locals__kernel_90eb52be 90eb52b load on device 'cuda:0' took 342.79 ms  (compiled)\n",
            "Module mujoco_warp._src.derivative fda8455 load on device 'cuda:0' took 520.73 ms  (compiled)\n",
            "\u001b[92m\u001b[0m\n",
            "\u001b[92m+---------------------------------+\n",
            "|         Base Environment        |\n",
            "+------------------------+--------+\n",
            "| Property               | Value  |\n",
            "+------------------------+--------+\n",
            "| Number of environments | 64     |\n",
            "| Environment device     | cuda:0 |\n",
            "| Environment seed       | 42     |\n",
            "| Physics step-size      | 0.02   |\n",
            "| Environment step-size  | 0.02   |\n",
            "+------------------------+--------+\u001b[0m\n",
            "\u001b[92m\u001b[0m\n",
            "\u001b[92m[INFO] <EventManager> contains 2 active terms.\n",
            "+-------------------------------------+\n",
            "| Active Event Terms in Mode: 'reset' |\n",
            "+--------+----------------------------+\n",
            "| Index  | Name                       |\n",
            "+--------+----------------------------+\n",
            "|   0    | reset_robot_joints         |\n",
            "+--------+----------------------------+\n",
            "+-----------------------------------------------+\n",
            "|     Active Event Terms in Mode: 'interval'    |\n",
            "+-------+-------------+-------------------------+\n",
            "| Index | Name        | Interval time range (s) |\n",
            "+-------+-------------+-------------------------+\n",
            "|   0   | random_push |        (1.0, 2.0)       |\n",
            "+-------+-------------+-------------------------+\n",
            "\u001b[0m\n",
            "\u001b[92m[INFO] <NullCommandManager> (inactive)\u001b[0m\n",
            "\u001b[92m[INFO] <ActionManager> contains 1 active terms.\n",
            "+--------------------------------+\n",
            "| Active Action Terms (shape: 1) |\n",
            "+-------+-----------+------------+\n",
            "| Index | Name      |  Dimension |\n",
            "+-------+-----------+------------+\n",
            "|   0   | joint_pos |          1 |\n",
            "+-------+-----------+------------+\n",
            "\u001b[0m\n",
            "\u001b[92m[INFO] <ObservationManager> contains 2 groups.\n",
            "+-----------------------------------------------------------+\n",
            "| Active Observation Terms in Group: 'policy' (shape: (4,)) |\n",
            "+----------------+------------------------+-----------------+\n",
            "|     Index      | Name                   |      Shape      |\n",
            "+----------------+------------------------+-----------------+\n",
            "|       0        | angle                  |       (1,)      |\n",
            "|       1        | ang_vel                |       (1,)      |\n",
            "|       2        | cart_pos               |       (1,)      |\n",
            "|       3        | cart_vel               |       (1,)      |\n",
            "+----------------+------------------------+-----------------+\n",
            "+-----------------------------------------------------------+\n",
            "| Active Observation Terms in Group: 'critic' (shape: (4,)) |\n",
            "+----------------+------------------------+-----------------+\n",
            "|     Index      | Name                   |      Shape      |\n",
            "+----------------+------------------------+-----------------+\n",
            "|       0        | angle                  |       (1,)      |\n",
            "|       1        | ang_vel                |       (1,)      |\n",
            "|       2        | cart_pos               |       (1,)      |\n",
            "|       3        | cart_vel               |       (1,)      |\n",
            "+----------------+------------------------+-----------------+\n",
            "\u001b[0m\n",
            "\u001b[92m[INFO] <TerminationManager> contains 2 active terms.\n",
            "+----------------------------+\n",
            "|  Active Termination Terms  |\n",
            "+-------+---------+----------+\n",
            "| Index | Name    | Time Out |\n",
            "+-------+---------+----------+\n",
            "|   0   | timeout |   True   |\n",
            "|   1   | tipped  |  False   |\n",
            "+-------+---------+----------+\n",
            "\u001b[0m\n",
            "\u001b[92m[INFO] <RewardManager> contains 2 active terms.\n",
            "+--------------------------+\n",
            "|   Active Reward Terms    |\n",
            "+-------+---------+--------+\n",
            "| Index | Name    | Weight |\n",
            "+-------+---------+--------+\n",
            "|   0   | upright |    5.0 |\n",
            "|   1   | effort  |    1.0 |\n",
            "+-------+---------+--------+\n",
            "\u001b[0m\n",
            "\u001b[92m[INFO] <NullCurriculumManager> (inactive)\u001b[0m\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ('policy',)\n",
            "\t critic :  ('critic',)\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "2025-12-05 18:12:12.479226: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764958332.495740     987 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764958332.500696     987 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764958332.513319     987 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764958332.513341     987 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764958332.513346     987 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764958332.513351     987 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-12-05 18:12:12.517372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mttktjmt\u001b[0m (\u001b[33mttktjmt-org\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m Waiting for wandb.init()...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.23.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/mjlab/wandb/run-20251205_181217-03ofai6l\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2025-12-05_18-11-25\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/ttktjmt-org/mjlab\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ttktjmt-org/mjlab/runs/03ofai6l\u001b[0m\n",
            "Could not find git repository in /usr/local/lib/python3.12/dist-packages/rsl_rl/__init__.py. Skipping.\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 0/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 1536 \n",
            "                       Steps per second: 1151 \n",
            "                        Collection time: 0.717s \n",
            "                          Learning time: 0.617s \n",
            "                        Mean value loss: 0.0816\n",
            "                    Mean surrogate loss: 0.0136\n",
            "                      Mean entropy loss: 1.4097\n",
            "                            Mean reward: 0.09\n",
            "                    Mean episode length: 8.69\n",
            "                  Mean action noise std: 0.97\n",
            "                 Episode_Reward/upright: 0.0688\n",
            "                  Episode_Reward/effort: -0.0620\n",
            "            Episode_Termination/timeout: 0.0833\n",
            "             Episode_Termination/tipped: 4.8750\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 1.33s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:04:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 1/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 3072 \n",
            "                       Steps per second: 5886 \n",
            "                        Collection time: 0.147s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0538\n",
            "                    Mean surrogate loss: -0.0333\n",
            "                      Mean entropy loss: 1.3163\n",
            "                            Mean reward: 0.09\n",
            "                    Mean episode length: 9.72\n",
            "                  Mean action noise std: 0.85\n",
            "                 Episode_Reward/upright: 0.0946\n",
            "                  Episode_Reward/effort: -0.0798\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 7.0417\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:02:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 2/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 4608 \n",
            "                       Steps per second: 5869 \n",
            "                        Collection time: 0.146s \n",
            "                          Learning time: 0.115s \n",
            "                        Mean value loss: 0.0411\n",
            "                    Mean surrogate loss: -0.0339\n",
            "                      Mean entropy loss: 1.1945\n",
            "                            Mean reward: 0.34\n",
            "                    Mean episode length: 9.55\n",
            "                  Mean action noise std: 0.76\n",
            "                 Episode_Reward/upright: 0.0965\n",
            "                  Episode_Reward/effort: -0.0650\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 6.7500\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:01\n",
            "                                    ETA: 00:02:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 3/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 6144 \n",
            "                       Steps per second: 6152 \n",
            "                        Collection time: 0.145s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0580\n",
            "                    Mean surrogate loss: -0.0168\n",
            "                      Mean entropy loss: 1.1373\n",
            "                            Mean reward: 0.46\n",
            "                    Mean episode length: 9.90\n",
            "                  Mean action noise std: 0.75\n",
            "                 Episode_Reward/upright: 0.0936\n",
            "                  Episode_Reward/effort: -0.0493\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 5.5417\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:01:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 4/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 7680 \n",
            "                       Steps per second: 5939 \n",
            "                        Collection time: 0.140s \n",
            "                          Learning time: 0.118s \n",
            "                        Mean value loss: 0.0847\n",
            "                    Mean surrogate loss: -0.0072\n",
            "                      Mean entropy loss: 1.1306\n",
            "                            Mean reward: 0.73\n",
            "                    Mean episode length: 14.41\n",
            "                  Mean action noise std: 0.75\n",
            "                 Episode_Reward/upright: 0.1594\n",
            "                  Episode_Reward/effort: -0.0778\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 3.2917\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:01:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 5/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 9216 \n",
            "                       Steps per second: 6425 \n",
            "                        Collection time: 0.132s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.1033\n",
            "                    Mean surrogate loss: -0.0048\n",
            "                      Mean entropy loss: 1.1152\n",
            "                            Mean reward: 0.90\n",
            "                    Mean episode length: 18.45\n",
            "                  Mean action noise std: 0.71\n",
            "                 Episode_Reward/upright: 0.2582\n",
            "                  Episode_Reward/effort: -0.1317\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.3333\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:01:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 6/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 10752 \n",
            "                       Steps per second: 6621 \n",
            "                        Collection time: 0.125s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0984\n",
            "                    Mean surrogate loss: -0.0076\n",
            "                      Mean entropy loss: 1.0330\n",
            "                            Mean reward: 1.19\n",
            "                    Mean episode length: 25.05\n",
            "                  Mean action noise std: 0.67\n",
            "                 Episode_Reward/upright: 0.4020\n",
            "                  Episode_Reward/effort: -0.2034\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.4167\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:02\n",
            "                                    ETA: 00:01:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 7/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 12288 \n",
            "                       Steps per second: 5887 \n",
            "                        Collection time: 0.131s \n",
            "                          Learning time: 0.130s \n",
            "                        Mean value loss: 0.1043\n",
            "                    Mean surrogate loss: -0.0108\n",
            "                      Mean entropy loss: 0.9649\n",
            "                            Mean reward: 1.60\n",
            "                    Mean episode length: 35.67\n",
            "                  Mean action noise std: 0.60\n",
            "                 Episode_Reward/upright: 0.5622\n",
            "                  Episode_Reward/effort: -0.3044\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.3750\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:01:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 8/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 13824 \n",
            "                       Steps per second: 4721 \n",
            "                        Collection time: 0.162s \n",
            "                          Learning time: 0.163s \n",
            "                        Mean value loss: 0.1032\n",
            "                    Mean surrogate loss: -0.0107\n",
            "                      Mean entropy loss: 0.8367\n",
            "                            Mean reward: 1.97\n",
            "                    Mean episode length: 43.35\n",
            "                  Mean action noise std: 0.53\n",
            "                 Episode_Reward/upright: 0.7062\n",
            "                  Episode_Reward/effort: -0.3468\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.4583\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.33s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:01:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                            Learning iteration 9/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 15360 \n",
            "                       Steps per second: 7199 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.1167\n",
            "                    Mean surrogate loss: -0.0096\n",
            "                      Mean entropy loss: 0.7418\n",
            "                            Mean reward: 2.21\n",
            "                    Mean episode length: 47.68\n",
            "                  Mean action noise std: 0.48\n",
            "                 Episode_Reward/upright: 0.8793\n",
            "                  Episode_Reward/effort: -0.4447\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:01:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 10/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 16896 \n",
            "                       Steps per second: 6926 \n",
            "                        Collection time: 0.113s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.1580\n",
            "                    Mean surrogate loss: 0.0036\n",
            "                      Mean entropy loss: 0.6308\n",
            "                            Mean reward: 2.54\n",
            "                    Mean episode length: 53.08\n",
            "                  Mean action noise std: 0.44\n",
            "                 Episode_Reward/upright: 0.7297\n",
            "                  Episode_Reward/effort: -0.2936\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0833\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:03\n",
            "                                    ETA: 00:01:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 11/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 18432 \n",
            "                       Steps per second: 7551 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.1528\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: 0.5938\n",
            "                            Mean reward: 2.67\n",
            "                    Mean episode length: 54.83\n",
            "                  Mean action noise std: 0.43\n",
            "                 Episode_Reward/upright: 0.9411\n",
            "                  Episode_Reward/effort: -0.3293\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:01:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 12/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 19968 \n",
            "                       Steps per second: 6856 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.1885\n",
            "                    Mean surrogate loss: -0.0080\n",
            "                      Mean entropy loss: 0.5573\n",
            "                            Mean reward: 3.14\n",
            "                    Mean episode length: 61.74\n",
            "                  Mean action noise std: 0.41\n",
            "                 Episode_Reward/upright: 1.1458\n",
            "                  Episode_Reward/effort: -0.4153\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0417\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:01:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 13/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 21504 \n",
            "                       Steps per second: 6970 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.2067\n",
            "                    Mean surrogate loss: -0.0017\n",
            "                      Mean entropy loss: 0.5008\n",
            "                            Mean reward: 4.02\n",
            "                    Mean episode length: 74.72\n",
            "                  Mean action noise std: 0.39\n",
            "                 Episode_Reward/upright: 1.4236\n",
            "                  Episode_Reward/effort: -0.5008\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.1250\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:59\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 14/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 23040 \n",
            "                       Steps per second: 7419 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.1282\n",
            "                    Mean surrogate loss: -0.0017\n",
            "                      Mean entropy loss: 0.4513\n",
            "                            Mean reward: 4.31\n",
            "                    Mean episode length: 78.80\n",
            "                  Mean action noise std: 0.37\n",
            "                 Episode_Reward/upright: 1.6161\n",
            "                  Episode_Reward/effort: -0.5843\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:58\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 15/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 24576 \n",
            "                       Steps per second: 7201 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.1832\n",
            "                    Mean surrogate loss: -0.0037\n",
            "                      Mean entropy loss: 0.3786\n",
            "                            Mean reward: 4.89\n",
            "                    Mean episode length: 86.68\n",
            "                  Mean action noise std: 0.34\n",
            "                 Episode_Reward/upright: 1.8287\n",
            "                  Episode_Reward/effort: -0.5389\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0417\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:04\n",
            "                                    ETA: 00:00:56\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 16/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 26112 \n",
            "                       Steps per second: 7591 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.2175\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: 0.3217\n",
            "                            Mean reward: 5.23\n",
            "                    Mean episode length: 91.27\n",
            "                  Mean action noise std: 0.33\n",
            "                 Episode_Reward/upright: 1.8653\n",
            "                  Episode_Reward/effort: -0.5254\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:55\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 17/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 27648 \n",
            "                       Steps per second: 7162 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.119s \n",
            "                        Mean value loss: 0.1957\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: 0.2777\n",
            "                            Mean reward: 5.55\n",
            "                    Mean episode length: 95.20\n",
            "                  Mean action noise std: 0.32\n",
            "                 Episode_Reward/upright: 2.1956\n",
            "                  Episode_Reward/effort: -0.5624\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.2500\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:54\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 18/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 29184 \n",
            "                       Steps per second: 7357 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.1123\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: 0.2614\n",
            "                            Mean reward: 5.70\n",
            "                    Mean episode length: 97.18\n",
            "                  Mean action noise std: 0.31\n",
            "                 Episode_Reward/upright: 2.4132\n",
            "                  Episode_Reward/effort: -0.6166\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0833\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:52\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 19/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 30720 \n",
            "                       Steps per second: 7612 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.1133\n",
            "                    Mean surrogate loss: -0.0049\n",
            "                      Mean entropy loss: 0.2173\n",
            "                            Mean reward: 5.93\n",
            "                    Mean episode length: 100.21\n",
            "                  Mean action noise std: 0.29\n",
            "                 Episode_Reward/upright: 2.9912\n",
            "                  Episode_Reward/effort: -0.8017\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:51\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 20/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 32256 \n",
            "                       Steps per second: 7700 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.1898\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: 0.1515\n",
            "                            Mean reward: 6.37\n",
            "                    Mean episode length: 105.57\n",
            "                  Mean action noise std: 0.28\n",
            "                 Episode_Reward/upright: 3.1463\n",
            "                  Episode_Reward/effort: -0.7370\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:05\n",
            "                                    ETA: 00:00:50\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 21/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 33792 \n",
            "                       Steps per second: 7653 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.1154\n",
            "                    Mean surrogate loss: -0.0005\n",
            "                      Mean entropy loss: 0.0840\n",
            "                            Mean reward: 6.37\n",
            "                    Mean episode length: 105.57\n",
            "                  Mean action noise std: 0.25\n",
            "                 Episode_Reward/upright: 3.0258\n",
            "                  Episode_Reward/effort: -0.6458\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:49\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 22/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 35328 \n",
            "                       Steps per second: 7325 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0612\n",
            "                    Mean surrogate loss: -0.0009\n",
            "                      Mean entropy loss: -0.0021\n",
            "                            Mean reward: 6.37\n",
            "                    Mean episode length: 105.57\n",
            "                  Mean action noise std: 0.23\n",
            "                 Episode_Reward/upright: 3.0258\n",
            "                  Episode_Reward/effort: -0.6458\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 23/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 36864 \n",
            "                       Steps per second: 7682 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.1053\n",
            "                    Mean surrogate loss: -0.0000\n",
            "                      Mean entropy loss: -0.0366\n",
            "                            Mean reward: 6.66\n",
            "                    Mean episode length: 108.99\n",
            "                  Mean action noise std: 0.23\n",
            "                 Episode_Reward/upright: 3.4639\n",
            "                  Episode_Reward/effort: -0.6971\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:48\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 24/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 38400 \n",
            "                       Steps per second: 7467 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0879\n",
            "                    Mean surrogate loss: 0.0004\n",
            "                      Mean entropy loss: -0.0391\n",
            "                            Mean reward: 7.25\n",
            "                    Mean episode length: 116.21\n",
            "                  Mean action noise std: 0.23\n",
            "                 Episode_Reward/upright: 4.1833\n",
            "                  Episode_Reward/effort: -0.9284\n",
            "            Episode_Termination/timeout: 0.0000\n",
            "             Episode_Termination/tipped: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:47\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 25/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 39936 \n",
            "                       Steps per second: 7455 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.1192\n",
            "                    Mean surrogate loss: -0.0037\n",
            "                      Mean entropy loss: -0.0524\n",
            "                            Mean reward: 7.94\n",
            "                    Mean episode length: 123.86\n",
            "                  Mean action noise std: 0.23\n",
            "                 Episode_Reward/upright: 2.1215\n",
            "                  Episode_Reward/effort: -0.2587\n",
            "            Episode_Termination/timeout: 0.1250\n",
            "             Episode_Termination/tipped: 0.8750\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:06\n",
            "                                    ETA: 00:00:46\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 26/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 41472 \n",
            "                       Steps per second: 7492 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0663\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: -0.0600\n",
            "                            Mean reward: 9.43\n",
            "                    Mean episode length: 142.13\n",
            "                  Mean action noise std: 0.22\n",
            "                 Episode_Reward/upright: 4.4130\n",
            "                  Episode_Reward/effort: -0.8648\n",
            "            Episode_Termination/timeout: 0.8333\n",
            "             Episode_Termination/tipped: 0.1667\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 27/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 43008 \n",
            "                       Steps per second: 7195 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0972\n",
            "                    Mean surrogate loss: 0.0043\n",
            "                      Mean entropy loss: -0.1030\n",
            "                            Mean reward: 11.30\n",
            "                    Mean episode length: 164.09\n",
            "                  Mean action noise std: 0.21\n",
            "                 Episode_Reward/upright: 4.8865\n",
            "                  Episode_Reward/effort: -0.9400\n",
            "            Episode_Termination/timeout: 0.8333\n",
            "             Episode_Termination/tipped: 0.1667\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:45\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 28/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 44544 \n",
            "                       Steps per second: 7360 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0544\n",
            "                    Mean surrogate loss: 0.0152\n",
            "                      Mean entropy loss: -0.1390\n",
            "                            Mean reward: 12.52\n",
            "                    Mean episode length: 177.56\n",
            "                  Mean action noise std: 0.21\n",
            "                 Episode_Reward/upright: 4.9518\n",
            "                  Episode_Reward/effort: -0.7621\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 29/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 46080 \n",
            "                       Steps per second: 7440 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.103s \n",
            "                        Mean value loss: 0.0742\n",
            "                    Mean surrogate loss: 0.0144\n",
            "                      Mean entropy loss: -0.1541\n",
            "                            Mean reward: 14.30\n",
            "                    Mean episode length: 196.68\n",
            "                  Mean action noise std: 0.21\n",
            "                 Episode_Reward/upright: 4.7892\n",
            "                  Episode_Reward/effort: -0.7116\n",
            "            Episode_Termination/timeout: 0.9167\n",
            "             Episode_Termination/tipped: 0.0833\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:07\n",
            "                                    ETA: 00:00:44\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 30/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 47616 \n",
            "                       Steps per second: 7321 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0467\n",
            "                    Mean surrogate loss: -0.0016\n",
            "                      Mean entropy loss: -0.1588\n",
            "                            Mean reward: 15.10\n",
            "                    Mean episode length: 205.19\n",
            "                  Mean action noise std: 0.20\n",
            "                 Episode_Reward/upright: 3.9095\n",
            "                  Episode_Reward/effort: -0.5568\n",
            "            Episode_Termination/timeout: 0.5000\n",
            "             Episode_Termination/tipped: 0.5000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 31/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 49152 \n",
            "                       Steps per second: 6821 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.119s \n",
            "                        Mean value loss: 0.0444\n",
            "                    Mean surrogate loss: -0.0049\n",
            "                      Mean entropy loss: -0.2329\n",
            "                            Mean reward: 17.63\n",
            "                    Mean episode length: 232.06\n",
            "                  Mean action noise std: 0.19\n",
            "                 Episode_Reward/upright: 4.9554\n",
            "                  Episode_Reward/effort: -0.5627\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:43\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 32/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 50688 \n",
            "                       Steps per second: 7615 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.1008\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: -0.2712\n",
            "                            Mean reward: 18.46\n",
            "                    Mean episode length: 240.44\n",
            "                  Mean action noise std: 0.18\n",
            "                 Episode_Reward/upright: 4.9561\n",
            "                  Episode_Reward/effort: -0.4942\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:42\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 33/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 52224 \n",
            "                       Steps per second: 7138 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0365\n",
            "                    Mean surrogate loss: -0.0002\n",
            "                      Mean entropy loss: -0.3519\n",
            "                            Mean reward: 21.74\n",
            "                    Mean episode length: 274.13\n",
            "                  Mean action noise std: 0.17\n",
            "                 Episode_Reward/upright: 4.9645\n",
            "                  Episode_Reward/effort: -0.4586\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:42\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 34/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 53760 \n",
            "                       Steps per second: 7182 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0277\n",
            "                    Mean surrogate loss: 0.0034\n",
            "                      Mean entropy loss: -0.4055\n",
            "                            Mean reward: 25.92\n",
            "                    Mean episode length: 316.59\n",
            "                  Mean action noise std: 0.16\n",
            "                 Episode_Reward/upright: 4.9607\n",
            "                  Episode_Reward/effort: -0.4204\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:08\n",
            "                                    ETA: 00:00:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 35/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 55296 \n",
            "                       Steps per second: 7648 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0262\n",
            "                    Mean surrogate loss: 0.0004\n",
            "                      Mean entropy loss: -0.4669\n",
            "                            Mean reward: 26.76\n",
            "                    Mean episode length: 324.98\n",
            "                  Mean action noise std: 0.16\n",
            "                 Episode_Reward/upright: 4.9712\n",
            "                  Episode_Reward/effort: -0.4138\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:41\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 36/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 56832 \n",
            "                       Steps per second: 7011 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0118\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: -0.4698\n",
            "                            Mean reward: 28.79\n",
            "                    Mean episode length: 345.42\n",
            "                  Mean action noise std: 0.15\n",
            "                 Episode_Reward/upright: 4.9681\n",
            "                  Episode_Reward/effort: -0.3759\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 37/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 58368 \n",
            "                       Steps per second: 7470 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0092\n",
            "                    Mean surrogate loss: 0.0100\n",
            "                      Mean entropy loss: -0.5110\n",
            "                            Mean reward: 30.06\n",
            "                    Mean episode length: 358.05\n",
            "                  Mean action noise std: 0.14\n",
            "                 Episode_Reward/upright: 4.9695\n",
            "                  Episode_Reward/effort: -0.3428\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 38/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 59904 \n",
            "                       Steps per second: 7659 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0118\n",
            "                    Mean surrogate loss: -0.0015\n",
            "                      Mean entropy loss: -0.5980\n",
            "                            Mean reward: 30.06\n",
            "                    Mean episode length: 358.05\n",
            "                  Mean action noise std: 0.14\n",
            "                 Episode_Reward/upright: 4.9704\n",
            "                  Episode_Reward/effort: -0.3205\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:40\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 39/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 61440 \n",
            "                       Steps per second: 7904 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0277\n",
            "                    Mean surrogate loss: 0.0086\n",
            "                      Mean entropy loss: -0.5691\n",
            "                            Mean reward: 30.06\n",
            "                    Mean episode length: 358.05\n",
            "                  Mean action noise std: 0.13\n",
            "                 Episode_Reward/upright: 4.9704\n",
            "                  Episode_Reward/effort: -0.3205\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.19s\n",
            "                           Time elapsed: 00:00:09\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 40/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 62976 \n",
            "                       Steps per second: 7629 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0165\n",
            "                    Mean surrogate loss: 0.0048\n",
            "                      Mean entropy loss: -0.6468\n",
            "                            Mean reward: 30.46\n",
            "                    Mean episode length: 362.02\n",
            "                  Mean action noise std: 0.13\n",
            "                 Episode_Reward/upright: 4.9597\n",
            "                  Episode_Reward/effort: -0.2594\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:39\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 41/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 64512 \n",
            "                       Steps per second: 6827 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.120s \n",
            "                        Mean value loss: 0.0178\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: -0.6879\n",
            "                            Mean reward: 31.29\n",
            "                    Mean episode length: 370.08\n",
            "                  Mean action noise std: 0.12\n",
            "                 Episode_Reward/upright: 4.9787\n",
            "                  Episode_Reward/effort: -0.2190\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 42/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 66048 \n",
            "                       Steps per second: 7479 \n",
            "                        Collection time: 0.091s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0089\n",
            "                    Mean surrogate loss: 0.0073\n",
            "                      Mean entropy loss: -0.7076\n",
            "                            Mean reward: 31.29\n",
            "                    Mean episode length: 370.08\n",
            "                  Mean action noise std: 0.12\n",
            "                 Episode_Reward/upright: 4.9807\n",
            "                  Episode_Reward/effort: -0.2043\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 43/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 67584 \n",
            "                       Steps per second: 5272 \n",
            "                        Collection time: 0.140s \n",
            "                          Learning time: 0.151s \n",
            "                        Mean value loss: 0.0058\n",
            "                    Mean surrogate loss: 0.0067\n",
            "                      Mean entropy loss: -0.7512\n",
            "                            Mean reward: 31.29\n",
            "                    Mean episode length: 370.08\n",
            "                  Mean action noise std: 0.11\n",
            "                 Episode_Reward/upright: 4.9807\n",
            "                  Episode_Reward/effort: -0.2043\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:10\n",
            "                                    ETA: 00:00:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 44/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 69120 \n",
            "                       Steps per second: 5914 \n",
            "                        Collection time: 0.117s \n",
            "                          Learning time: 0.142s \n",
            "                        Mean value loss: 0.0123\n",
            "                    Mean surrogate loss: 0.0111\n",
            "                      Mean entropy loss: -0.8171\n",
            "                            Mean reward: 31.71\n",
            "                    Mean episode length: 374.11\n",
            "                  Mean action noise std: 0.11\n",
            "                 Episode_Reward/upright: 4.9878\n",
            "                  Episode_Reward/effort: -0.2207\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:38\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 45/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 70656 \n",
            "                       Steps per second: 5669 \n",
            "                        Collection time: 0.128s \n",
            "                          Learning time: 0.143s \n",
            "                        Mean value loss: 0.0076\n",
            "                    Mean surrogate loss: 0.0126\n",
            "                      Mean entropy loss: -0.7518\n",
            "                            Mean reward: 32.89\n",
            "                    Mean episode length: 385.06\n",
            "                  Mean action noise std: 0.10\n",
            "                 Episode_Reward/upright: 4.9838\n",
            "                  Episode_Reward/effort: -0.1898\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 46/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 72192 \n",
            "                       Steps per second: 6171 \n",
            "                        Collection time: 0.120s \n",
            "                          Learning time: 0.129s \n",
            "                        Mean value loss: 0.0070\n",
            "                    Mean surrogate loss: 0.0016\n",
            "                      Mean entropy loss: -0.8848\n",
            "                            Mean reward: 33.67\n",
            "                    Mean episode length: 392.06\n",
            "                  Mean action noise std: 0.10\n",
            "                 Episode_Reward/upright: 4.9856\n",
            "                  Episode_Reward/effort: -0.1765\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 47/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 73728 \n",
            "                       Steps per second: 5673 \n",
            "                        Collection time: 0.132s \n",
            "                          Learning time: 0.139s \n",
            "                        Mean value loss: 0.0096\n",
            "                    Mean surrogate loss: -0.0052\n",
            "                      Mean entropy loss: -0.9710\n",
            "                            Mean reward: 35.68\n",
            "                    Mean episode length: 411.19\n",
            "                  Mean action noise std: 0.09\n",
            "                 Episode_Reward/upright: 4.9896\n",
            "                  Episode_Reward/effort: -0.1769\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:11\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 48/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 75264 \n",
            "                       Steps per second: 5745 \n",
            "                        Collection time: 0.125s \n",
            "                          Learning time: 0.142s \n",
            "                        Mean value loss: 0.0068\n",
            "                    Mean surrogate loss: 0.0037\n",
            "                      Mean entropy loss: -0.9984\n",
            "                            Mean reward: 37.28\n",
            "                    Mean episode length: 426.14\n",
            "                  Mean action noise std: 0.08\n",
            "                 Episode_Reward/upright: 4.9904\n",
            "                  Episode_Reward/effort: -0.1507\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 49/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 76800 \n",
            "                       Steps per second: 5554 \n",
            "                        Collection time: 0.124s \n",
            "                          Learning time: 0.153s \n",
            "                        Mean value loss: 0.0051\n",
            "                    Mean surrogate loss: 0.0093\n",
            "                      Mean entropy loss: -1.0576\n",
            "                            Mean reward: 38.84\n",
            "                    Mean episode length: 439.91\n",
            "                  Mean action noise std: 0.08\n",
            "                 Episode_Reward/upright: 4.9899\n",
            "                  Episode_Reward/effort: -0.1434\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 50/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 78336 \n",
            "                       Steps per second: 5202 \n",
            "                        Collection time: 0.132s \n",
            "                          Learning time: 0.163s \n",
            "                        Mean value loss: 0.0016\n",
            "                    Mean surrogate loss: 0.0164\n",
            "                      Mean entropy loss: -1.0700\n",
            "                            Mean reward: 40.37\n",
            "                    Mean episode length: 453.91\n",
            "                  Mean action noise std: 0.09\n",
            "                 Episode_Reward/upright: 4.9895\n",
            "                  Episode_Reward/effort: -0.1331\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:12\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 51/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 79872 \n",
            "                       Steps per second: 5112 \n",
            "                        Collection time: 0.138s \n",
            "                          Learning time: 0.163s \n",
            "                        Mean value loss: 0.0047\n",
            "                    Mean surrogate loss: -0.0017\n",
            "                      Mean entropy loss: -1.0190\n",
            "                            Mean reward: 41.39\n",
            "                    Mean episode length: 462.32\n",
            "                  Mean action noise std: 0.08\n",
            "                 Episode_Reward/upright: 4.9871\n",
            "                  Episode_Reward/effort: -0.1278\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:37\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 52/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 81408 \n",
            "                       Steps per second: 5910 \n",
            "                        Collection time: 0.148s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0050\n",
            "                    Mean surrogate loss: 0.0017\n",
            "                      Mean entropy loss: -1.1182\n",
            "                            Mean reward: 43.14\n",
            "                    Mean episode length: 477.78\n",
            "                  Mean action noise std: 0.08\n",
            "                 Episode_Reward/upright: 4.9896\n",
            "                  Episode_Reward/effort: -0.1076\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 53/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 82944 \n",
            "                       Steps per second: 7427 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0021\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: -1.1549\n",
            "                            Mean reward: 44.70\n",
            "                    Mean episode length: 489.60\n",
            "                  Mean action noise std: 0.08\n",
            "                 Episode_Reward/upright: 4.9884\n",
            "                  Episode_Reward/effort: -0.1008\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 54/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 84480 \n",
            "                       Steps per second: 7040 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0009\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: -1.2027\n",
            "                            Mean reward: 45.98\n",
            "                    Mean episode length: 497.74\n",
            "                  Mean action noise std: 0.08\n",
            "                 Episode_Reward/upright: 4.9930\n",
            "                  Episode_Reward/effort: -0.0923\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:36\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 55/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 86016 \n",
            "                       Steps per second: 7274 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0012\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: -1.1763\n",
            "                            Mean reward: 46.63\n",
            "                    Mean episode length: 497.92\n",
            "                  Mean action noise std: 0.07\n",
            "                 Episode_Reward/upright: 4.9937\n",
            "                  Episode_Reward/effort: -0.0888\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:13\n",
            "                                    ETA: 00:00:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 56/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 87552 \n",
            "                       Steps per second: 7006 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.116s \n",
            "                        Mean value loss: 0.0026\n",
            "                    Mean surrogate loss: 0.0099\n",
            "                      Mean entropy loss: -1.2099\n",
            "                            Mean reward: 47.02\n",
            "                    Mean episode length: 497.92\n",
            "                  Mean action noise std: 0.08\n",
            "                 Episode_Reward/upright: 4.9943\n",
            "                  Episode_Reward/effort: -0.0830\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 57/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 89088 \n",
            "                       Steps per second: 7545 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0025\n",
            "                    Mean surrogate loss: -0.0022\n",
            "                      Mean entropy loss: -1.1611\n",
            "                            Mean reward: 47.35\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.08\n",
            "                 Episode_Reward/upright: 4.9944\n",
            "                  Episode_Reward/effort: -0.0767\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:35\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 58/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 90624 \n",
            "                       Steps per second: 7473 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0009\n",
            "                    Mean surrogate loss: 0.0033\n",
            "                      Mean entropy loss: -1.1873\n",
            "                            Mean reward: 47.46\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.07\n",
            "                 Episode_Reward/upright: 4.9957\n",
            "                  Episode_Reward/effort: -0.0729\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 59/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 92160 \n",
            "                       Steps per second: 7509 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0017\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: -1.2461\n",
            "                            Mean reward: 47.46\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.07\n",
            "                 Episode_Reward/upright: 4.9963\n",
            "                  Episode_Reward/effort: -0.0688\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 60/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 93696 \n",
            "                       Steps per second: 7701 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0007\n",
            "                    Mean surrogate loss: -0.0059\n",
            "                      Mean entropy loss: -1.3580\n",
            "                            Mean reward: 47.51\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.06\n",
            "                 Episode_Reward/upright: 4.9961\n",
            "                  Episode_Reward/effort: -0.0694\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:14\n",
            "                                    ETA: 00:00:34\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 61/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 95232 \n",
            "                       Steps per second: 6981 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.125s \n",
            "                        Mean value loss: 0.0011\n",
            "                    Mean surrogate loss: 0.0004\n",
            "                      Mean entropy loss: -1.4127\n",
            "                            Mean reward: 47.51\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.06\n",
            "                 Episode_Reward/upright: 4.9947\n",
            "                  Episode_Reward/effort: -0.0737\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 62/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 96768 \n",
            "                       Steps per second: 7279 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0015\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -1.4761\n",
            "                            Mean reward: 47.61\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9965\n",
            "                  Episode_Reward/effort: -0.0538\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 63/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 98304 \n",
            "                       Steps per second: 7867 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0004\n",
            "                    Mean surrogate loss: 0.0090\n",
            "                      Mean entropy loss: -1.6149\n",
            "                            Mean reward: 47.61\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9968\n",
            "                  Episode_Reward/effort: -0.0513\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:33\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 64/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 99840 \n",
            "                       Steps per second: 7625 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0010\n",
            "                    Mean surrogate loss: 0.0053\n",
            "                      Mean entropy loss: -1.5904\n",
            "                            Mean reward: 47.61\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9968\n",
            "                  Episode_Reward/effort: -0.0513\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 65/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 101376 \n",
            "                       Steps per second: 7258 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0019\n",
            "                    Mean surrogate loss: 0.0082\n",
            "                      Mean entropy loss: -1.6660\n",
            "                            Mean reward: 47.76\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9971\n",
            "                  Episode_Reward/effort: -0.0484\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:15\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 66/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 102912 \n",
            "                       Steps per second: 7411 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0007\n",
            "                    Mean surrogate loss: 0.0132\n",
            "                      Mean entropy loss: -1.7015\n",
            "                            Mean reward: 47.81\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.04\n",
            "                 Episode_Reward/upright: 4.9975\n",
            "                  Episode_Reward/effort: -0.0453\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:32\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 67/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 104448 \n",
            "                       Steps per second: 7494 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0006\n",
            "                    Mean surrogate loss: 0.0041\n",
            "                      Mean entropy loss: -1.7022\n",
            "                            Mean reward: 47.96\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.04\n",
            "                 Episode_Reward/upright: 4.9977\n",
            "                  Episode_Reward/effort: -0.0392\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 68/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 105984 \n",
            "                       Steps per second: 7615 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0018\n",
            "                    Mean surrogate loss: 0.0125\n",
            "                      Mean entropy loss: -1.6796\n",
            "                            Mean reward: 48.14\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9979\n",
            "                  Episode_Reward/effort: -0.0363\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 69/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 107520 \n",
            "                       Steps per second: 7315 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0007\n",
            "                    Mean surrogate loss: 0.0061\n",
            "                      Mean entropy loss: -1.7515\n",
            "                            Mean reward: 48.31\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.04\n",
            "                 Episode_Reward/upright: 4.9966\n",
            "                  Episode_Reward/effort: -0.0352\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:16\n",
            "                                    ETA: 00:00:31\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 70/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 109056 \n",
            "                       Steps per second: 7496 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0007\n",
            "                    Mean surrogate loss: 0.0048\n",
            "                      Mean entropy loss: -1.6953\n",
            "                            Mean reward: 48.48\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.04\n",
            "                 Episode_Reward/upright: 4.9973\n",
            "                  Episode_Reward/effort: -0.0313\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 71/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 110592 \n",
            "                       Steps per second: 6771 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0006\n",
            "                    Mean surrogate loss: 0.0027\n",
            "                      Mean entropy loss: -1.6677\n",
            "                            Mean reward: 48.64\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9966\n",
            "                  Episode_Reward/effort: -0.0328\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 72/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 112128 \n",
            "                       Steps per second: 7297 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: 0.0030\n",
            "                      Mean entropy loss: -1.6225\n",
            "                            Mean reward: 48.76\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9974\n",
            "                  Episode_Reward/effort: -0.0293\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 73/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 113664 \n",
            "                       Steps per second: 7177 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0003\n",
            "                    Mean surrogate loss: -0.0021\n",
            "                      Mean entropy loss: -1.6415\n",
            "                            Mean reward: 48.97\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9976\n",
            "                  Episode_Reward/effort: -0.0278\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:30\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 74/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 115200 \n",
            "                       Steps per second: 7363 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: -0.0048\n",
            "                      Mean entropy loss: -1.6648\n",
            "                            Mean reward: 49.11\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9976\n",
            "                  Episode_Reward/effort: -0.0263\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:17\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 75/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 116736 \n",
            "                       Steps per second: 6689 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.121s \n",
            "                        Mean value loss: 0.0002\n",
            "                    Mean surrogate loss: 0.0023\n",
            "                      Mean entropy loss: -1.6630\n",
            "                            Mean reward: 49.25\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9968\n",
            "                  Episode_Reward/effort: -0.0268\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 76/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 118272 \n",
            "                       Steps per second: 7437 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0004\n",
            "                    Mean surrogate loss: 0.0073\n",
            "                      Mean entropy loss: -1.6627\n",
            "                            Mean reward: 49.31\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.05\n",
            "                 Episode_Reward/upright: 4.9975\n",
            "                  Episode_Reward/effort: -0.0255\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:29\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 77/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 119808 \n",
            "                       Steps per second: 7371 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0005\n",
            "                    Mean surrogate loss: -0.0037\n",
            "                      Mean entropy loss: -1.7243\n",
            "                            Mean reward: 49.39\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.04\n",
            "                 Episode_Reward/upright: 4.9972\n",
            "                  Episode_Reward/effort: -0.0232\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 78/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 121344 \n",
            "                       Steps per second: 7607 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: -0.0021\n",
            "                      Mean entropy loss: -1.7565\n",
            "                            Mean reward: 49.40\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.04\n",
            "                 Episode_Reward/upright: 4.9974\n",
            "                  Episode_Reward/effort: -0.0243\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 79/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 122880 \n",
            "                       Steps per second: 7115 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: -0.0044\n",
            "                      Mean entropy loss: -1.8128\n",
            "                            Mean reward: 49.42\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.04\n",
            "                 Episode_Reward/upright: 4.9976\n",
            "                  Episode_Reward/effort: -0.0205\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:18\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 80/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 124416 \n",
            "                       Steps per second: 6849 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.117s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: -0.0059\n",
            "                      Mean entropy loss: -1.8517\n",
            "                            Mean reward: 49.42\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.04\n",
            "                 Episode_Reward/upright: 4.9977\n",
            "                  Episode_Reward/effort: -0.0201\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:28\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 81/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 125952 \n",
            "                       Steps per second: 7364 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.111s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0060\n",
            "                      Mean entropy loss: -1.8926\n",
            "                            Mean reward: 49.43\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.03\n",
            "                 Episode_Reward/upright: 4.9974\n",
            "                  Episode_Reward/effort: -0.0192\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 82/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 127488 \n",
            "                       Steps per second: 7199 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.119s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: -1.9600\n",
            "                            Mean reward: 49.44\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.03\n",
            "                 Episode_Reward/upright: 4.9965\n",
            "                  Episode_Reward/effort: -0.0173\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 83/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 129024 \n",
            "                       Steps per second: 7479 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: -2.0262\n",
            "                            Mean reward: 49.45\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.03\n",
            "                 Episode_Reward/upright: 4.9976\n",
            "                  Episode_Reward/effort: -0.0174\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 84/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 130560 \n",
            "                       Steps per second: 7375 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.118s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: -0.0009\n",
            "                      Mean entropy loss: -2.0615\n",
            "                            Mean reward: 49.45\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.03\n",
            "                 Episode_Reward/upright: 4.9978\n",
            "                  Episode_Reward/effort: -0.0171\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:19\n",
            "                                    ETA: 00:00:27\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 85/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 132096 \n",
            "                       Steps per second: 7461 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: -0.0011\n",
            "                      Mean entropy loss: -2.1863\n",
            "                            Mean reward: 49.47\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.03\n",
            "                 Episode_Reward/upright: 4.9977\n",
            "                  Episode_Reward/effort: -0.0172\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 86/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 133632 \n",
            "                       Steps per second: 7419 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: -2.1750\n",
            "                            Mean reward: 49.49\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.03\n",
            "                 Episode_Reward/upright: 4.9968\n",
            "                  Episode_Reward/effort: -0.0179\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 87/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 135168 \n",
            "                       Steps per second: 7672 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0035\n",
            "                      Mean entropy loss: -2.3067\n",
            "                            Mean reward: 49.49\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9966\n",
            "                  Episode_Reward/effort: -0.0169\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:26\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 88/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 136704 \n",
            "                       Steps per second: 7335 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0057\n",
            "                      Mean entropy loss: -2.3449\n",
            "                            Mean reward: 49.52\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9968\n",
            "                  Episode_Reward/effort: -0.0156\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:20\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 89/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 138240 \n",
            "                       Steps per second: 6970 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.114s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0040\n",
            "                      Mean entropy loss: -2.4028\n",
            "                            Mean reward: 49.56\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9955\n",
            "                  Episode_Reward/effort: -0.0168\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 90/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 139776 \n",
            "                       Steps per second: 7417 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0038\n",
            "                      Mean entropy loss: -2.3832\n",
            "                            Mean reward: 49.59\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9970\n",
            "                  Episode_Reward/effort: -0.0130\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 91/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 141312 \n",
            "                       Steps per second: 7558 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: -2.4526\n",
            "                            Mean reward: 49.62\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9966\n",
            "                  Episode_Reward/effort: -0.0144\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:25\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 92/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 142848 \n",
            "                       Steps per second: 7176 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0057\n",
            "                      Mean entropy loss: -2.5343\n",
            "                            Mean reward: 49.65\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9959\n",
            "                  Episode_Reward/effort: -0.0148\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 93/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 144384 \n",
            "                       Steps per second: 7320 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: -2.6092\n",
            "                            Mean reward: 49.67\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9974\n",
            "                  Episode_Reward/effort: -0.0113\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:21\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 94/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 145920 \n",
            "                       Steps per second: 6624 \n",
            "                        Collection time: 0.126s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0012\n",
            "                      Mean entropy loss: -2.6609\n",
            "                            Mean reward: 49.72\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9970\n",
            "                  Episode_Reward/effort: -0.0125\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 95/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 147456 \n",
            "                       Steps per second: 7483 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0007\n",
            "                      Mean entropy loss: -2.7627\n",
            "                            Mean reward: 49.74\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.02\n",
            "                 Episode_Reward/upright: 4.9974\n",
            "                  Episode_Reward/effort: -0.0093\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:24\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 96/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 148992 \n",
            "                       Steps per second: 6324 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.127s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0019\n",
            "                      Mean entropy loss: -2.8547\n",
            "                            Mean reward: 49.77\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9974\n",
            "                  Episode_Reward/effort: -0.0106\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 97/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 150528 \n",
            "                       Steps per second: 5306 \n",
            "                        Collection time: 0.144s \n",
            "                          Learning time: 0.145s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0004\n",
            "                      Mean entropy loss: -2.8987\n",
            "                            Mean reward: 49.78\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0097\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:22\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 98/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 152064 \n",
            "                       Steps per second: 5604 \n",
            "                        Collection time: 0.136s \n",
            "                          Learning time: 0.138s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0027\n",
            "                      Mean entropy loss: -2.8856\n",
            "                            Mean reward: 49.79\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9969\n",
            "                  Episode_Reward/effort: -0.0098\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 99/200                            \u001b[0m \n",
            "\n",
            "                            Total steps: 153600 \n",
            "                       Steps per second: 6015 \n",
            "                        Collection time: 0.116s \n",
            "                          Learning time: 0.140s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0074\n",
            "                      Mean entropy loss: -2.9604\n",
            "                            Mean reward: 49.80\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9977\n",
            "                  Episode_Reward/effort: -0.0082\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 100/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 155136 \n",
            "                       Steps per second: 6249 \n",
            "                        Collection time: 0.112s \n",
            "                          Learning time: 0.134s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0065\n",
            "                      Mean entropy loss: -2.9257\n",
            "                            Mean reward: 49.80\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9963\n",
            "                  Episode_Reward/effort: -0.0114\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:23\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 101/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 156672 \n",
            "                       Steps per second: 5813 \n",
            "                        Collection time: 0.112s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0183\n",
            "                      Mean entropy loss: -2.9958\n",
            "                            Mean reward: 49.80\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9963\n",
            "                  Episode_Reward/effort: -0.0116\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:23\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 102/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 158208 \n",
            "                       Steps per second: 6036 \n",
            "                        Collection time: 0.124s \n",
            "                          Learning time: 0.131s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: -2.9597\n",
            "                            Mean reward: 49.80\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9977\n",
            "                  Episode_Reward/effort: -0.0089\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 103/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 159744 \n",
            "                       Steps per second: 5455 \n",
            "                        Collection time: 0.132s \n",
            "                          Learning time: 0.150s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0004\n",
            "                      Mean entropy loss: -3.0552\n",
            "                            Mean reward: 49.80\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0058\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 104/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 161280 \n",
            "                       Steps per second: 5317 \n",
            "                        Collection time: 0.129s \n",
            "                          Learning time: 0.159s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0045\n",
            "                      Mean entropy loss: -3.0732\n",
            "                            Mean reward: 49.80\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:24\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 105/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 162816 \n",
            "                       Steps per second: 4920 \n",
            "                        Collection time: 0.129s \n",
            "                          Learning time: 0.183s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0093\n",
            "                      Mean entropy loss: -3.0644\n",
            "                            Mean reward: 49.80\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.31s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:22\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 106/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 164352 \n",
            "                       Steps per second: 6290 \n",
            "                        Collection time: 0.133s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0048\n",
            "                      Mean entropy loss: -3.1194\n",
            "                            Mean reward: 49.81\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9980\n",
            "                  Episode_Reward/effort: -0.0073\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.24s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 107/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 165888 \n",
            "                       Steps per second: 7670 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0002\n",
            "                      Mean entropy loss: -3.1382\n",
            "                            Mean reward: 49.81\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9975\n",
            "                  Episode_Reward/effort: -0.0095\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 108/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 167424 \n",
            "                       Steps per second: 7665 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0057\n",
            "                      Mean entropy loss: -3.1648\n",
            "                            Mean reward: 49.81\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0064\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 109/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 168960 \n",
            "                       Steps per second: 7571 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0036\n",
            "                      Mean entropy loss: -3.2056\n",
            "                            Mean reward: 49.82\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0064\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:25\n",
            "                                    ETA: 00:00:21\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 110/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 170496 \n",
            "                       Steps per second: 7082 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.103s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0018\n",
            "                      Mean entropy loss: -3.1998\n",
            "                            Mean reward: 49.83\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9987\n",
            "                  Episode_Reward/effort: -0.0064\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 111/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 172032 \n",
            "                       Steps per second: 7516 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: -3.3170\n",
            "                            Mean reward: 49.84\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 112/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 173568 \n",
            "                       Steps per second: 7246 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0020\n",
            "                      Mean entropy loss: -3.3698\n",
            "                            Mean reward: 49.85\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0041\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 113/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 175104 \n",
            "                       Steps per second: 7341 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0089\n",
            "                      Mean entropy loss: -3.3700\n",
            "                            Mean reward: 49.86\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9989\n",
            "                  Episode_Reward/effort: -0.0057\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:20\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 114/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 176640 \n",
            "                       Steps per second: 7037 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.116s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0109\n",
            "                      Mean entropy loss: -3.4184\n",
            "                            Mean reward: 49.87\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9984\n",
            "                  Episode_Reward/effort: -0.0055\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:26\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 115/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 178176 \n",
            "                       Steps per second: 7457 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0014\n",
            "                      Mean entropy loss: -3.4612\n",
            "                            Mean reward: 49.87\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0058\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 116/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 179712 \n",
            "                       Steps per second: 7068 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0001\n",
            "                      Mean entropy loss: -3.4912\n",
            "                            Mean reward: 49.88\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9989\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 117/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 181248 \n",
            "                       Steps per second: 7033 \n",
            "                        Collection time: 0.113s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0060\n",
            "                      Mean entropy loss: -3.6164\n",
            "                            Mean reward: 49.90\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0054\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:19\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 118/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 182784 \n",
            "                       Steps per second: 7247 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.115s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0024\n",
            "                      Mean entropy loss: -3.6455\n",
            "                            Mean reward: 49.90\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0061\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 119/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 184320 \n",
            "                       Steps per second: 6947 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0041\n",
            "                      Mean entropy loss: -3.6384\n",
            "                            Mean reward: 49.90\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9984\n",
            "                  Episode_Reward/effort: -0.0064\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:27\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 120/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 185856 \n",
            "                       Steps per second: 7384 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0012\n",
            "                      Mean entropy loss: -3.6672\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 121/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 187392 \n",
            "                       Steps per second: 7078 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.116s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0017\n",
            "                      Mean entropy loss: -3.7117\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:18\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 122/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 188928 \n",
            "                       Steps per second: 7359 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0041\n",
            "                      Mean entropy loss: -3.6918\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 123/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 190464 \n",
            "                       Steps per second: 7066 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.122s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: -3.7451\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:28\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 124/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 192000 \n",
            "                       Steps per second: 7596 \n",
            "                        Collection time: 0.094s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0053\n",
            "                      Mean entropy loss: -3.8150\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 125/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 193536 \n",
            "                       Steps per second: 7549 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0079\n",
            "                      Mean entropy loss: -3.8175\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0023\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:17\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 126/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 195072 \n",
            "                       Steps per second: 7868 \n",
            "                        Collection time: 0.089s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0034\n",
            "                      Mean entropy loss: -3.8554\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0023\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 127/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 196608 \n",
            "                       Steps per second: 7419 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0020\n",
            "                      Mean entropy loss: -3.8773\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0027\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 128/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 198144 \n",
            "                       Steps per second: 6811 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.130s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0007\n",
            "                      Mean entropy loss: -3.8988\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9983\n",
            "                  Episode_Reward/effort: -0.0066\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:29\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 129/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 199680 \n",
            "                       Steps per second: 7281 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0079\n",
            "                      Mean entropy loss: -3.8562\n",
            "                            Mean reward: 49.91\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9978\n",
            "                  Episode_Reward/effort: -0.0077\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:00:16\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 130/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 201216 \n",
            "                       Steps per second: 7567 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0070\n",
            "                      Mean entropy loss: -3.8431\n",
            "                            Mean reward: 49.92\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 131/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 202752 \n",
            "                       Steps per second: 7020 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0116\n",
            "                      Mean entropy loss: -3.7959\n",
            "                            Mean reward: 49.92\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.01\n",
            "                 Episode_Reward/upright: 4.9992\n",
            "                  Episode_Reward/effort: -0.0046\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 132/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 204288 \n",
            "                       Steps per second: 7688 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0046\n",
            "                      Mean entropy loss: -3.8683\n",
            "                            Mean reward: 49.92\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 133/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 205824 \n",
            "                       Steps per second: 6802 \n",
            "                        Collection time: 0.117s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: -3.8913\n",
            "                            Mean reward: 49.93\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0053\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:30\n",
            "                                    ETA: 00:00:15\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 134/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 207360 \n",
            "                       Steps per second: 7273 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.115s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0049\n",
            "                      Mean entropy loss: -3.9561\n",
            "                            Mean reward: 49.93\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9989\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:31\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 135/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 208896 \n",
            "                       Steps per second: 7172 \n",
            "                        Collection time: 0.109s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0022\n",
            "                      Mean entropy loss: -3.9494\n",
            "                            Mean reward: 49.93\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:31\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 136/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 210432 \n",
            "                       Steps per second: 7581 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0080\n",
            "                      Mean entropy loss: -3.9117\n",
            "                            Mean reward: 49.93\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9984\n",
            "                  Episode_Reward/effort: -0.0070\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:31\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 137/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 211968 \n",
            "                       Steps per second: 6149 \n",
            "                        Collection time: 0.110s \n",
            "                          Learning time: 0.140s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0022\n",
            "                      Mean entropy loss: -4.0978\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0055\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:31\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 138/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 213504 \n",
            "                       Steps per second: 6938 \n",
            "                        Collection time: 0.115s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: -4.1203\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9985\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:00:14\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 139/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 215040 \n",
            "                       Steps per second: 7574 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0103\n",
            "                      Mean entropy loss: -4.0657\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9992\n",
            "                  Episode_Reward/effort: -0.0028\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 140/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 216576 \n",
            "                       Steps per second: 7487 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0021\n",
            "                      Mean entropy loss: -4.0688\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9979\n",
            "                  Episode_Reward/effort: -0.0063\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 141/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 218112 \n",
            "                       Steps per second: 7403 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0014\n",
            "                      Mean entropy loss: -4.1194\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 142/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 219648 \n",
            "                       Steps per second: 7071 \n",
            "                        Collection time: 0.107s \n",
            "                          Learning time: 0.110s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0147\n",
            "                      Mean entropy loss: -4.0089\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0022\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:32\n",
            "                                    ETA: 00:00:13\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 143/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 221184 \n",
            "                       Steps per second: 7324 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0029\n",
            "                      Mean entropy loss: -4.0908\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0022\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 144/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 222720 \n",
            "                       Steps per second: 7500 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0303\n",
            "                      Mean entropy loss: -4.2936\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9981\n",
            "                  Episode_Reward/effort: -0.0077\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 145/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 224256 \n",
            "                       Steps per second: 7234 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.117s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0034\n",
            "                      Mean entropy loss: -4.2805\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9985\n",
            "                  Episode_Reward/effort: -0.0067\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 146/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 225792 \n",
            "                       Steps per second: 7542 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -4.2918\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9992\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:00:12\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 147/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 227328 \n",
            "                       Steps per second: 7257 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0090\n",
            "                      Mean entropy loss: -4.2801\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9992\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:33\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 148/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 228864 \n",
            "                       Steps per second: 7659 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0066\n",
            "                      Mean entropy loss: -4.2402\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0026\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:34\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 149/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 230400 \n",
            "                       Steps per second: 7524 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0057\n",
            "                      Mean entropy loss: -4.2224\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:34\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 150/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 231936 \n",
            "                       Steps per second: 7603 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0108\n",
            "                      Mean entropy loss: -4.1993\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9982\n",
            "                  Episode_Reward/effort: -0.0062\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:34\n",
            "                                    ETA: 00:00:11\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 151/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 233472 \n",
            "                       Steps per second: 5110 \n",
            "                        Collection time: 0.138s \n",
            "                          Learning time: 0.162s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0013\n",
            "                      Mean entropy loss: -4.2095\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9989\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:34\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 152/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 235008 \n",
            "                       Steps per second: 5775 \n",
            "                        Collection time: 0.127s \n",
            "                          Learning time: 0.139s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0004\n",
            "                      Mean entropy loss: -4.2817\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.27s\n",
            "                           Time elapsed: 00:00:35\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 153/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 236544 \n",
            "                       Steps per second: 6043 \n",
            "                        Collection time: 0.120s \n",
            "                          Learning time: 0.134s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0030\n",
            "                      Mean entropy loss: -4.3456\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9992\n",
            "                  Episode_Reward/effort: -0.0036\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:35\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 154/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 238080 \n",
            "                       Steps per second: 5985 \n",
            "                        Collection time: 0.128s \n",
            "                          Learning time: 0.129s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0113\n",
            "                      Mean entropy loss: -4.4036\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0039\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:35\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 155/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 239616 \n",
            "                       Steps per second: 5404 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.170s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0025\n",
            "                      Mean entropy loss: -4.3856\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.28s\n",
            "                           Time elapsed: 00:00:35\n",
            "                                    ETA: 00:00:10\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 156/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 241152 \n",
            "                       Steps per second: 5386 \n",
            "                        Collection time: 0.131s \n",
            "                          Learning time: 0.154s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0017\n",
            "                      Mean entropy loss: -4.3944\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.29s\n",
            "                           Time elapsed: 00:00:36\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 157/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 242688 \n",
            "                       Steps per second: 6052 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.140s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0012\n",
            "                      Mean entropy loss: -4.3699\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0059\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:36\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 158/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 244224 \n",
            "                       Steps per second: 5155 \n",
            "                        Collection time: 0.146s \n",
            "                          Learning time: 0.152s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0038\n",
            "                      Mean entropy loss: -4.4497\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.30s\n",
            "                           Time elapsed: 00:00:36\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 159/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 245760 \n",
            "                       Steps per second: 4586 \n",
            "                        Collection time: 0.168s \n",
            "                          Learning time: 0.167s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0005\n",
            "                      Mean entropy loss: -4.4771\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.1250\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.33s\n",
            "                           Time elapsed: 00:00:37\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 160/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 247296 \n",
            "                       Steps per second: 5827 \n",
            "                        Collection time: 0.140s \n",
            "                          Learning time: 0.124s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0038\n",
            "                      Mean entropy loss: -4.3768\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9992\n",
            "                  Episode_Reward/effort: -0.0037\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.26s\n",
            "                           Time elapsed: 00:00:37\n",
            "                                    ETA: 00:00:09\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 161/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 248832 \n",
            "                       Steps per second: 7312 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.109s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0076\n",
            "                      Mean entropy loss: -4.3412\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:37\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 162/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 250368 \n",
            "                       Steps per second: 7446 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0039\n",
            "                      Mean entropy loss: -4.2895\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0054\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:37\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 163/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 251904 \n",
            "                       Steps per second: 7447 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0048\n",
            "                      Mean entropy loss: -4.3255\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:37\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 164/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 253440 \n",
            "                       Steps per second: 7531 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0003\n",
            "                      Mean entropy loss: -4.3454\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0035\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:00:08\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 165/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 254976 \n",
            "                       Steps per second: 7806 \n",
            "                        Collection time: 0.093s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0082\n",
            "                      Mean entropy loss: -4.3623\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0068\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 166/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 256512 \n",
            "                       Steps per second: 7277 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.112s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0023\n",
            "                      Mean entropy loss: -4.3895\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9984\n",
            "                  Episode_Reward/effort: -0.0063\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 167/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 258048 \n",
            "                       Steps per second: 7326 \n",
            "                        Collection time: 0.097s \n",
            "                          Learning time: 0.113s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0025\n",
            "                      Mean entropy loss: -4.3912\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9979\n",
            "                  Episode_Reward/effort: -0.0069\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 168/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 259584 \n",
            "                       Steps per second: 7017 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0020\n",
            "                      Mean entropy loss: -4.3661\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9979\n",
            "                  Episode_Reward/effort: -0.0069\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:38\n",
            "                                    ETA: 00:00:07\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 169/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 261120 \n",
            "                       Steps per second: 7685 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.103s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0165\n",
            "                      Mean entropy loss: -4.4472\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9982\n",
            "                  Episode_Reward/effort: -0.0050\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:39\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 170/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 262656 \n",
            "                       Steps per second: 7736 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.103s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0034\n",
            "                      Mean entropy loss: -4.4009\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0032\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:39\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 171/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 264192 \n",
            "                       Steps per second: 7401 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0099\n",
            "                      Mean entropy loss: -4.3694\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0033\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:39\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 172/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 265728 \n",
            "                       Steps per second: 6944 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.119s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0021\n",
            "                      Mean entropy loss: -4.3613\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9989\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:39\n",
            "                                    ETA: 00:00:06\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 173/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 267264 \n",
            "                       Steps per second: 7461 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0139\n",
            "                      Mean entropy loss: -4.3747\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 174/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 268800 \n",
            "                       Steps per second: 7447 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0073\n",
            "                      Mean entropy loss: -4.3728\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0038\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 175/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 270336 \n",
            "                       Steps per second: 7470 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0068\n",
            "                      Mean entropy loss: -4.3357\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9994\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 176/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 271872 \n",
            "                       Steps per second: 7790 \n",
            "                        Collection time: 0.096s \n",
            "                          Learning time: 0.101s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -4.3255\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 177/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 273408 \n",
            "                       Steps per second: 6062 \n",
            "                        Collection time: 0.114s \n",
            "                          Learning time: 0.139s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0037\n",
            "                      Mean entropy loss: -4.3200\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9992\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.25s\n",
            "                           Time elapsed: 00:00:40\n",
            "                                    ETA: 00:00:05\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 178/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 274944 \n",
            "                       Steps per second: 7293 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0055\n",
            "                      Mean entropy loss: -4.3712\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:41\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 179/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 276480 \n",
            "                       Steps per second: 7316 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0098\n",
            "                      Mean entropy loss: -4.3816\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0040\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:41\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 180/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 278016 \n",
            "                       Steps per second: 6998 \n",
            "                        Collection time: 0.111s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0011\n",
            "                      Mean entropy loss: -4.3895\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0044\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:41\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 181/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 279552 \n",
            "                       Steps per second: 6812 \n",
            "                        Collection time: 0.106s \n",
            "                          Learning time: 0.120s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0023\n",
            "                      Mean entropy loss: -4.3767\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9992\n",
            "                  Episode_Reward/effort: -0.0049\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.23s\n",
            "                           Time elapsed: 00:00:41\n",
            "                                    ETA: 00:00:04\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 182/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 281088 \n",
            "                       Steps per second: 7408 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0068\n",
            "                      Mean entropy loss: -4.3193\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:41\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 183/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 282624 \n",
            "                       Steps per second: 7428 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0031\n",
            "                      Mean entropy loss: -4.4075\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0054\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:42\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 184/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 284160 \n",
            "                       Steps per second: 7715 \n",
            "                        Collection time: 0.092s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0050\n",
            "                      Mean entropy loss: -4.3988\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0069\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:42\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 185/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 285696 \n",
            "                       Steps per second: 7479 \n",
            "                        Collection time: 0.098s \n",
            "                          Learning time: 0.108s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0062\n",
            "                      Mean entropy loss: -4.3274\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9987\n",
            "                  Episode_Reward/effort: -0.0066\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:42\n",
            "                                    ETA: 00:00:03\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 186/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 287232 \n",
            "                       Steps per second: 7383 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.118s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0088\n",
            "                      Mean entropy loss: -4.3490\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9995\n",
            "                  Episode_Reward/effort: -0.0048\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:42\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 187/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 288768 \n",
            "                       Steps per second: 7498 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0001\n",
            "                      Mean entropy loss: -4.3351\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9994\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:42\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 188/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 290304 \n",
            "                       Steps per second: 7515 \n",
            "                        Collection time: 0.090s \n",
            "                          Learning time: 0.115s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0093\n",
            "                      Mean entropy loss: -4.2993\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9994\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:43\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 189/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 291840 \n",
            "                       Steps per second: 7624 \n",
            "                        Collection time: 0.095s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0044\n",
            "                      Mean entropy loss: -4.2904\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9994\n",
            "                  Episode_Reward/effort: -0.0042\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.20s\n",
            "                           Time elapsed: 00:00:43\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 190/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 293376 \n",
            "                       Steps per second: 7489 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0003\n",
            "                      Mean entropy loss: -4.2644\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9988\n",
            "                  Episode_Reward/effort: -0.0046\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:43\n",
            "                                    ETA: 00:00:02\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 191/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 294912 \n",
            "                       Steps per second: 7137 \n",
            "                        Collection time: 0.108s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: -0.0019\n",
            "                      Mean entropy loss: -4.3071\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0063\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:43\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 192/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 296448 \n",
            "                       Steps per second: 7412 \n",
            "                        Collection time: 0.101s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0136\n",
            "                      Mean entropy loss: -4.3493\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9991\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:43\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 193/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 297984 \n",
            "                       Steps per second: 7145 \n",
            "                        Collection time: 0.099s \n",
            "                          Learning time: 0.116s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0051\n",
            "                      Mean entropy loss: -4.3503\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9986\n",
            "                  Episode_Reward/effort: -0.0047\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:44\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 194/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 299520 \n",
            "                       Steps per second: 7393 \n",
            "                        Collection time: 0.103s \n",
            "                          Learning time: 0.104s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0028\n",
            "                      Mean entropy loss: -4.3342\n",
            "                            Mean reward: 49.95\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9993\n",
            "                  Episode_Reward/effort: -0.0043\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:44\n",
            "                                    ETA: 00:00:01\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 195/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 301056 \n",
            "                       Steps per second: 7136 \n",
            "                        Collection time: 0.100s \n",
            "                          Learning time: 0.116s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: -0.0010\n",
            "                      Mean entropy loss: -4.3636\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0052\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.22s\n",
            "                           Time elapsed: 00:00:44\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 196/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 302592 \n",
            "                       Steps per second: 7286 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.105s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: 0.0003\n",
            "                      Mean entropy loss: -4.3735\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9990\n",
            "                  Episode_Reward/effort: -0.0045\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:44\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 197/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 304128 \n",
            "                       Steps per second: 7384 \n",
            "                        Collection time: 0.102s \n",
            "                          Learning time: 0.106s \n",
            "                        Mean value loss: 0.0001\n",
            "                    Mean surrogate loss: -0.0014\n",
            "                      Mean entropy loss: -4.3091\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9985\n",
            "                  Episode_Reward/effort: -0.0055\n",
            "            Episode_Termination/timeout: 1.0000\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:45\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 198/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 305664 \n",
            "                       Steps per second: 7266 \n",
            "                        Collection time: 0.105s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0014\n",
            "                      Mean entropy loss: -4.2912\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9985\n",
            "                  Episode_Reward/effort: -0.0059\n",
            "            Episode_Termination/timeout: 1.0833\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:45\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "################################################################################\n",
            "\u001b[1m                           Learning iteration 199/200                           \u001b[0m \n",
            "\n",
            "                            Total steps: 307200 \n",
            "                       Steps per second: 7263 \n",
            "                        Collection time: 0.104s \n",
            "                          Learning time: 0.107s \n",
            "                        Mean value loss: 0.0000\n",
            "                    Mean surrogate loss: 0.0038\n",
            "                      Mean entropy loss: -4.3069\n",
            "                            Mean reward: 49.94\n",
            "                    Mean episode length: 500.00\n",
            "                  Mean action noise std: 0.00\n",
            "                 Episode_Reward/upright: 4.9994\n",
            "                  Episode_Reward/effort: -0.0034\n",
            "            Episode_Termination/timeout: 1.0417\n",
            "             Episode_Termination/tipped: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "                         Iteration time: 0.21s\n",
            "                           Time elapsed: 00:00:45\n",
            "                                    ETA: 00:00:00\n",
            "\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33m2025-12-05_18-11-25\u001b[0m at: \u001b[34mhttps://wandb.ai/ttktjmt-org/mjlab/runs/03ofai6l\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20251205_181217-03ofai6l/logs\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# This will take several minutes depending on your training configuration\n",
        "# !uv run train Mjlab-Cartpole --agent.max-iterations 200 --agent.save-interval 20\n",
        "# !python ./src/mjlab/scripts/train.py Mjlab-Cartpole --help\n",
        "!python /content/mjlab/src/mjlab/scripts/train.py Mjlab-Cartpole --agent.max-iterations 200 --agent.save-interval 20"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCaqPznGrx8H"
      },
      "source": [
        "### **üìÅ Locate Training Checkpoints**\n",
        "\n",
        "After training, checkpoints are saved locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPnmHYu8r0uY",
        "outputId": "f1e65385-3e7a-495a-8bdf-0ce6de863450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Latest training run: 2025-12-05_18-11-25\n",
            "\n",
            "Found 11 checkpoints:\n",
            "  ‚Ä¢ model_199.pt (1.00 MB)\n",
            "  ‚Ä¢ model_20.pt (1.00 MB)\n",
            "  ‚Ä¢ model_40.pt (1.00 MB)\n",
            "  ‚Ä¢ model_60.pt (1.00 MB)\n",
            "  ‚Ä¢ model_80.pt (1.00 MB)\n",
            "\n",
            "üíæ Last checkpoint: logs/rsl_rl/cartpole/2025-12-05_18-11-25/model_80.pt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Find the most recent training run\n",
        "log_dir = Path(\"logs/rsl_rl/cartpole\")\n",
        "if log_dir.exists():\n",
        "    runs = sorted(log_dir.glob(\"*\"), key=os.path.getmtime, reverse=True)\n",
        "    if runs:\n",
        "        latest_run = runs[0]\n",
        "        print(f\"‚úì Latest training run: {latest_run.name}\\n\")\n",
        "\n",
        "        # List checkpoints\n",
        "        checkpoints = sorted(latest_run.glob(\"model_*.pt\"))\n",
        "        if checkpoints:\n",
        "            print(f\"Found {len(checkpoints)} checkpoints:\")\n",
        "            for ckpt in checkpoints[-5:]:  # Show last 5\n",
        "                size_mb = ckpt.stat().st_size / (1024 * 1024)\n",
        "                print(f\"  ‚Ä¢ {ckpt.name} ({size_mb:.2f} MB)\")\n",
        "\n",
        "            # Store the last checkpoint path\n",
        "            last_checkpoint = str(checkpoints[-1])\n",
        "            print(f\"\\nüíæ Last checkpoint: {last_checkpoint}\")\n",
        "        else:\n",
        "            print(\"‚ö† No checkpoints found yet\")\n",
        "    else:\n",
        "        print(\"‚ö† No training runs found\")\n",
        "else:\n",
        "    print(\"‚ö† Log directory not found. Have you run training yet?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWFS9Pw7r2uH"
      },
      "source": [
        "---\n",
        "\n",
        "## **üéÆ Step 4: Visualize the Trained Policy**\n",
        "\n",
        "Let's see the trained policy in action!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78PgHtpfr5sb"
      },
      "source": [
        "### **üåê Launch Viser API**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9tGiFyBr2bW",
        "outputId": "24a51cf1-8bef-404d-891e-efb99842dd89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO]: Loading checkpoint: model_80.pt\n",
            "Warp 1.10.1 initialized:\n",
            "   CUDA Toolkit 12.8, Driver 12.4\n",
            "   Devices:\n",
            "     \"cpu\"      : \"x86_64\"\n",
            "     \"cuda:0\"   : \"Tesla T4\" (15 GiB, sm_75, mempool enabled)\n",
            "   Kernel cache:\n",
            "     /root/.cache/warp/1.10.1\n",
            "Module mujoco_warp._src.smooth 9ca7ec0 load on device 'cuda:0' took 3.94 ms  (cached)\n",
            "Module mujoco_warp._src.collision_driver e72006d load on device 'cuda:0' took 0.39 ms  (cached)\n",
            "Module _nxn_broadphase__locals__kernel_1799b5b8 1799b5b load on device 'cuda:0' took 0.30 ms  (cached)\n",
            "Module mujoco_warp._src.collision_primitive._create_narrowphase_kernel f53bec7 load on device 'cuda:0' took 0.38 ms  (cached)\n",
            "Module mujoco_warp._src.constraint fa42ba8 load on device 'cuda:0' took 1.02 ms  (cached)\n",
            "Module _actuator_velocity__locals__actuator_velocity_7933d235 876a329 load on device 'cuda:0' took 0.33 ms  (cached)\n",
            "Module mujoco_warp._src.passive fc4f8e1 load on device 'cuda:0' took 0.51 ms  (cached)\n",
            "Module mujoco_warp._src.forward a88f545 load on device 'cuda:0' took 0.64 ms  (cached)\n",
            "Module mujoco_warp._src.support 769a44d load on device 'cuda:0' took 0.31 ms  (cached)\n",
            "Module _tile_cholesky_factorize_solve__locals__cholesky_factorize_solve_43a10bd2 d7e04e1 load on device 'cuda:0' took 31.84 ms  (cached)\n",
            "Module mujoco_warp._src.solver 1699532 load on device 'cuda:0' took 0.54 ms  (cached)\n",
            "Module mul_m_dense__locals___mul_m_dense_34acc5d2 6f70d43 load on device 'cuda:0' took 27.45 ms  (cached)\n",
            "Module update_constraint_gauss_cost__locals__kernel_fd0aa713 fd0aa71 load on device 'cuda:0' took 0.32 ms  (cached)\n",
            "Module update_gradient_JTDAJ_dense_tiled__locals__kernel_fcb58f0e 913eb00 load on device 'cuda:0' took 29.32 ms  (cached)\n",
            "Module update_gradient_cholesky__locals__kernel_b2b7bfef 2ca8f88 load on device 'cuda:0' took 38.58 ms  (cached)\n",
            "Module linesearch_jv_fused__locals__kernel_90eb52be 90eb52b load on device 'cuda:0' took 0.40 ms  (cached)\n",
            "Module mujoco_warp._src.derivative fda8455 load on device 'cuda:0' took 0.35 ms  (cached)\n",
            "\n",
            "+---------------------------------+\n",
            "|         Base Environment        |\n",
            "+------------------------+--------+\n",
            "| Property               | Value  |\n",
            "+------------------------+--------+\n",
            "| Number of environments | 4      |\n",
            "| Environment device     | cuda:0 |\n",
            "| Environment seed       | None   |\n",
            "| Physics step-size      | 0.02   |\n",
            "| Environment step-size  | 0.02   |\n",
            "+------------------------+--------+\n",
            "\n",
            "[INFO] <EventManager> contains 1 active terms.\n",
            "+-------------------------------------+\n",
            "| Active Event Terms in Mode: 'reset' |\n",
            "+--------+----------------------------+\n",
            "| Index  | Name                       |\n",
            "+--------+----------------------------+\n",
            "|   0    | reset_robot_joints         |\n",
            "+--------+----------------------------+\n",
            "\n",
            "[INFO] <NullCommandManager> (inactive)\n",
            "[INFO] <ActionManager> contains 1 active terms.\n",
            "+--------------------------------+\n",
            "| Active Action Terms (shape: 1) |\n",
            "+-------+-----------+------------+\n",
            "| Index | Name      |  Dimension |\n",
            "+-------+-----------+------------+\n",
            "|   0   | joint_pos |          1 |\n",
            "+-------+-----------+------------+\n",
            "\n",
            "[INFO] <ObservationManager> contains 2 groups.\n",
            "+-----------------------------------------------------------+\n",
            "| Active Observation Terms in Group: 'policy' (shape: (4,)) |\n",
            "+----------------+------------------------+-----------------+\n",
            "|     Index      | Name                   |      Shape      |\n",
            "+----------------+------------------------+-----------------+\n",
            "|       0        | angle                  |       (1,)      |\n",
            "|       1        | ang_vel                |       (1,)      |\n",
            "|       2        | cart_pos               |       (1,)      |\n",
            "|       3        | cart_vel               |       (1,)      |\n",
            "+----------------+------------------------+-----------------+\n",
            "+-----------------------------------------------------------+\n",
            "| Active Observation Terms in Group: 'critic' (shape: (4,)) |\n",
            "+----------------+------------------------+-----------------+\n",
            "|     Index      | Name                   |      Shape      |\n",
            "+----------------+------------------------+-----------------+\n",
            "|       0        | angle                  |       (1,)      |\n",
            "|       1        | ang_vel                |       (1,)      |\n",
            "|       2        | cart_pos               |       (1,)      |\n",
            "|       3        | cart_vel               |       (1,)      |\n",
            "+----------------+------------------------+-----------------+\n",
            "\n",
            "[INFO] <TerminationManager> contains 2 active terms.\n",
            "+----------------------------+\n",
            "|  Active Termination Terms  |\n",
            "+-------+---------+----------+\n",
            "| Index | Name    | Time Out |\n",
            "+-------+---------+----------+\n",
            "|   0   | timeout |   True   |\n",
            "|   1   | tipped  |  False   |\n",
            "+-------+---------+----------+\n",
            "\n",
            "[INFO] <RewardManager> contains 2 active terms.\n",
            "+--------------------------+\n",
            "|   Active Reward Terms    |\n",
            "+-------+---------+--------+\n",
            "| Index | Name    | Weight |\n",
            "+-------+---------+--------+\n",
            "|   0   | upright |    5.0 |\n",
            "|   1   | effort  |    1.0 |\n",
            "+-------+---------+--------+\n",
            "\n",
            "[INFO] <NullCurriculumManager> (inactive)\n",
            "--------------------------------------------------------------------------------\n",
            "Resolved observation sets: \n",
            "\t policy :  ('policy',)\n",
            "\t critic :  ('critic',)\n",
            "--------------------------------------------------------------------------------\n",
            "Actor MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "Critic MLP: MLP(\n",
            "  (0): Linear(in_features=4, out_features=256, bias=True)\n",
            "  (1): ELU(alpha=1.0)\n",
            "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (3): ELU(alpha=1.0)\n",
            "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (5): ELU(alpha=1.0)\n",
            "  (6): Linear(in_features=64, out_features=1, bias=True)\n",
            ")\n",
            "‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ viser (listening *:8081) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n",
            "\n",
            "====================================================\n",
            "‚úÖ Server is running! Execute the next cell to view.\n",
            "====================================================\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "process = subprocess.Popen(\n",
        "  [\n",
        "    \"python\",\n",
        "    \"/content/mjlab/src/mjlab/scripts/play.py\",\n",
        "    \"Mjlab-Cartpole\",\n",
        "    \"--checkpoint_file\",\n",
        "    last_checkpoint,\n",
        "    \"--num_envs\",\n",
        "    \"4\",\n",
        "  ],\n",
        "  stdout=subprocess.PIPE,\n",
        "  stderr=subprocess.STDOUT,\n",
        "  universal_newlines=True,\n",
        "  bufsize=1,\n",
        ")\n",
        "\n",
        "for line in process.stdout:\n",
        "  print(line, end=\"\")\n",
        "  sys.stdout.flush()\n",
        "\n",
        "  if \"serving\" in line.lower() or \"running on\" in line.lower() or \"8081\" in line:\n",
        "    print(\"\\n\" + \"=\" * 52)\n",
        "    print(\"‚úÖ Server is running! Execute the next cell to view.\")\n",
        "    print(\"=\" * 52)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üñ•Ô∏è Embed Client as iframe**"
      ],
      "metadata": {
        "id": "XgzJXyBXssZS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "ll89QnuSuUxx",
        "outputId": "b6c041d7-71b3-430a-c789-be6976d03058"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8081, \"/\", \"100%\", 700, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import output\n",
        "\n",
        "output.serve_kernel_port_as_iframe(\n",
        "    port=8081,\n",
        "    height=700\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}